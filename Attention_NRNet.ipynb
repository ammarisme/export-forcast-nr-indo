{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Attention_NRNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarisme/export-forecast-NR/blob/master/Attention_NRNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C83XgaWprQnn",
        "colab_type": "code",
        "outputId": "b2efde1b-a5c6-4b41-fa9c-81f516feb5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.5.0.html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-cluster==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.3)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.0+cu101)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (3.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.4.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Installing collected packages: torch-sparse, torch-scatter, torch-cluster\n",
            "  Found existing installation: torch-sparse 0.6.4\n",
            "    Uninstalling torch-sparse-0.6.4:\n",
            "      Successfully uninstalled torch-sparse-0.6.4\n",
            "  Found existing installation: torch-scatter 2.0.4\n",
            "    Uninstalling torch-scatter-2.0.4:\n",
            "      Successfully uninstalled torch-scatter-2.0.4\n",
            "  Found existing installation: torch-cluster 1.5.4\n",
            "    Uninstalling torch-cluster-1.5.4:\n",
            "      Successfully uninstalled torch-cluster-1.5.4\n",
            "Successfully installed torch-cluster-1.5.4 torch-scatter-2.0.4 torch-sparse-0.6.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch_cluster",
                  "torch_scatter",
                  "torch_sparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKz91FjrQn3",
        "colab_type": "code",
        "outputId": "a601d8a2-9ca2-4e41-977d-64ac468fedab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
        "\n",
        "import numpy as np\n",
        "from numpy import isnan\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")           \n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device, torch.cuda.is_available())\n",
        "print('number of cuda devices : ', torch.cuda.device_count())\n",
        "print('memory allocated : ', torch.cuda.memory_allocated(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0 True\n",
            "number of cuda devices :  1\n",
            "memory allocated :  110097920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFsXKb1CzwSY",
        "colab_type": "code",
        "outputId": "59c4526c-11d1-457c-e412-b24aa97890d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os.path\n",
        "from os import path\n",
        "import sys\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/My Drive/rubber'\n",
        "INPUT_ROOT = PATH+'/input_mix/test'\n",
        "sys.path.append(PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA4KbKSVr1Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Common function\n",
        "\"\"\"\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "  \n",
        "\n",
        "def save_model_parameters(model_parameters_dict, models = None, fileName=None):\n",
        "  if fileName == None:\n",
        "    fileName = 'model_parameters_'+str(np.round(np.random.randn(1000,200)[0][0]*1000))\n",
        "\n",
        "  model_summary = pd.DataFrame(columns =[])\n",
        "  for i in sorted(model_parameters_dict.keys()):\n",
        "    param_size = 0\n",
        "    if models != None:\n",
        "      param_size = get_n_params(models[i][0])\n",
        "      param_size += get_n_params(models[i][1])\n",
        "    row = pd.Series({\n",
        "                        'model_id':i,\n",
        "                        'encoder repeats' : model_parameters_dict[i][0]['repeats'],\n",
        "                        'decoder repeats' : model_parameters_dict[i][1]['repeats'],\n",
        "                        'parameters' :param_size\n",
        "    })\n",
        "    row_df = pd.DataFrame([row], index = [i])\n",
        "    model_summary = pd.concat([model_summary, row_df])\n",
        "  model_summary.to_csv(fileName)\n",
        "\n",
        "#Generate the list of repeat count parameters list\n",
        "\n",
        "#generate_model_parameters(generate_repeat_counts())\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "def save_dict(dictionary, filename):\n",
        "  np.save(filename, dictionary)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points, points2):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(points)\n",
        "    plt.plot(points2)\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def latest_model(mypath):\n",
        "  model = Encoder(**model_configurations[0][0])\n",
        "  fnames= []\n",
        "  for fname in listdir(mypath):\n",
        "    if \"cleaned_non_window\" in fname:\n",
        "      fnames.append(int(fname.split('_')[0]))\n",
        "  print('Model path: ', mypath+'/'+str(min(fnames))+'_cleaned_non_window.model')\n",
        "  model = torch.load(mypath+'/'+str(min(fnames))+'_cleaned_non_window.model')\n",
        "  return model\n",
        "\n",
        "def get_latest_model(mypath):\n",
        "  model = Encoder(**model_configurations[0][0])\n",
        "  model = torch.load(mypath+'/latest.model')\n",
        "  optimizer = torch.load(mypath+'/latest.optim')\n",
        "  return model, optimizer\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HidtRwtG9CFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqDataSet(InMemoryDataset):\n",
        "    def __init__(self, root, input_sequence, output_sequence, transform=None, pre_transform=None):\n",
        "        super(Seq2SeqDataSet, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "      if os.path.exists(self.root+PROCESSED_DIR):\n",
        "        return self.root+CLEANED_DIR #'/cleaned'\n",
        "      else:\n",
        "        os.mkdir(self.root+PROCESSED_DIR)\n",
        "        return self.root+CLEANED_DIR#'/cleaned'\n",
        "        \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "      if os.path.exists(self.root+PROCESSED_DIR):\n",
        "        return self.root+PROCESSED_DIR\n",
        "      else:\n",
        "        os.mkdir(self.root+PROCESSED_DIR)\n",
        "        return self.root+PROCESSED_DIR\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "      mypath = self.raw_dir\n",
        "      filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "      return filenames\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed.dt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self):\n",
        "      output_size , output_sequence_len, input_sequence_len= configuration['output_size'],\\\n",
        "      configuration['output_sequence_len'],\\\n",
        "      configuration['input_sequence_len'],\n",
        "        \n",
        "      data_list = []\n",
        "\n",
        "      for raw_path in self.raw_paths:\n",
        "        print(raw_path)\n",
        "        df = pd.read_csv(raw_path)\n",
        "        for synthetic_seq in df['synthesis_seq'].unique():\n",
        "          synthetic_data = df[df['synthesis_seq']==synthetic_seq]\n",
        "\n",
        "          country_data = synthetic_data[features]\n",
        "                      \n",
        "          #del country_data['synthesis_seq']\n",
        "          x_columns = [feature for feature in features if feature != 'target']\n",
        "          country_data_i = country_data[x_columns][:-output_sequence_len]\n",
        "          country_data_array = country_data_i.to_numpy()\n",
        "          feature_length = len(x_columns)\n",
        "          x = country_data_array#[:feature_length].T\n",
        "\n",
        "          country_data_o = country_data[input_sequence_len:]\n",
        "          #country_data_array = country_data_array.reshape(feature_length,len(country_data_i))\n",
        "          country_data_array_y = np.array([country_data_o['target'].to_numpy()])\n",
        "          country_data_array_y = country_data_array_y.reshape(output_size,len(country_data_o))\n",
        "          y = country_data_array_y[:output_size].T\n",
        "\n",
        "          sets =0\n",
        "          x_list = []\n",
        "          dict_x = dict()\n",
        "          for i in range(input_sequence_len):\n",
        "            array_len = (len(x)-i) - ((len(x)  - i)%input_sequence_len)+i\n",
        "            if array_len <= 0:\n",
        "              print('skipping')\n",
        "              continue\n",
        "            sets = int( array_len/ input_sequence_len)\n",
        "            if sets <= 0:\n",
        "              print('skipping')\n",
        "              continue\n",
        "            #print(len(x))\n",
        "            #print('input seq : ', i , ' ', array_len , ' ',array_len-i , ' number of sets : ', sets)\n",
        "            x_temp = x[i:array_len].T.reshape(sets, feature_length, input_sequence_len)\n",
        "            uniq_keys = np.array([i+k*input_sequence_len for k in range(sets)])\n",
        "            x_temp = x_temp.reshape(feature_length,sets,input_sequence_len)\n",
        "            arrays_split = np.hsplit(x_temp,sets)\n",
        "            dict_x.update(dict(zip(uniq_keys, arrays_split)))\n",
        "\n",
        "          dict_y = dict()\n",
        "          y_list = []\n",
        "          for i in range(output_sequence_len):\n",
        "            array_len_y = (len(y)-i) - ((len(y)  - i)%output_sequence_len)+i\n",
        "            if array_len_y <= 0:\n",
        "              continue\n",
        "            sets = int(array_len_y / output_sequence_len)\n",
        "            if sets <= 0:\n",
        "              continue\n",
        "            y_temp = y[i:array_len_y].T.reshape(sets, output_size, output_sequence_len)\n",
        "            #uniq_keys = np.array([i+(output_sequence_len*k) for k in range(output_sequence_len)])\n",
        "            uniq_keys = np.array([i+k*output_sequence_len for k in range(sets)])\n",
        "            y_temp = y_temp.reshape(output_size,sets,output_sequence_len)\n",
        "            arrays_split = np.hsplit(y_temp,sets)\n",
        "            dict_y.update(dict(zip(uniq_keys, arrays_split)))\n",
        "\n",
        "          temp_x_list = []\n",
        "          mean = np.mean(country_data[x_columns].to_numpy(), axis=0).T\n",
        "          std = np.std(country_data[x_columns].to_numpy(), axis=0).T\n",
        "          #print(sorted(dict_x.keys()))\n",
        "          for i in sorted(dict_x.keys()):\n",
        "            x = dict_x[i].squeeze()\n",
        "            #print(x.T[0][12])\n",
        "            x = (x.T - mean) / std\n",
        "            where_are_NaNs = isnan(x)\n",
        "            x[where_are_NaNs] = 0\n",
        "            temp_x_list.append(x)\n",
        "\n",
        "          temp_y_list  = [dict_y[i].T for i in sorted(dict_y.keys())]\n",
        "\n",
        "          #_country_code,popData2018\n",
        "          xy_list = [Data(x = torch.from_numpy(features).type(torch.FloatTensor).squeeze()) for features in temp_x_list]\n",
        "          \n",
        "          mean_y = np.mean(y)\n",
        "          std_y = np.std(y)\n",
        "          for i in sorted(dict_y.keys()):\n",
        "            temp_y_list[i] = temp_y_list[i].squeeze()\n",
        "            norm_y = (temp_y_list[i] -mean_y)/std_y\n",
        "            xy_list[i].y = torch.from_numpy(norm_y).squeeze()\n",
        "\n",
        "          data_list += xy_list\n",
        "        print('processed : '+ raw_path)\n",
        "      data, slices = self.collate(data_list)\n",
        "      torch.save((data, slices), self.processed_paths[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgVv8X46_7S0",
        "colab_type": "code",
        "outputId": "f4ab40c6-09a8-4adf-d97b-7efcbbd5dbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def get_selected_features():\n",
        "  original_dataset = pd.read_csv(INPUT_ROOT+'/validation/cleaned/all_hs_codes_combined.csv')\n",
        "  features = ['target']\n",
        "  #features.extend([column for column in original_dataset.columns if ((\"4001\" in column) or (\"4003\" in column) or (\"4002\" in column)) and ((\"China\" in column) or (\"Japan\" in column))])\n",
        "  features.extend([column for column in original_dataset.columns if (\"target\" not in column)])\n",
        "  return features\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "INPUT_ROOT = PATH+'/input_mix/test'\n",
        "\n",
        "features = get_selected_features()\n",
        "print(features)\n",
        "configuration = {\n",
        "    'input_sequence_len' : 12,\n",
        "    'output_sequence_len' : 12,\n",
        "    'training_batch_size' : 2048,\n",
        "    'training_dataset_length' :32768,\n",
        "    'validation_batch_size' : 2,\n",
        "    'yhat_size' : 1,\n",
        "    'feature_len' : len(features)-1,\n",
        "    'output_size' : 1,\n",
        "}\n",
        "\n",
        "DATA_TAG = \"window_hs_seq2seq_\"+str(configuration['input_sequence_len'])+'_'+str(configuration['output_sequence_len'])\n",
        "PROCESSED_DIR = '/processed_'+DATA_TAG\n",
        "CLEANED_DIR =  '/cleaned'\n",
        "\n",
        "\n",
        "validation_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['output_sequence_len'])\n",
        "validation_dataset = validation_dataset.shuffle()\n",
        "validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n",
        "\n",
        "print(validation_dataset[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "['target', 'Albania_4001', 'Albania_4002', 'Albania_4003', 'Albania_4004', 'Albania_4005', 'Albania_4006', 'Albania_4007', 'Albania_4008', 'Albania_4009', 'Albania_4010', 'Albania_4011', 'Albania_4012', 'Albania_4013', 'Albania_4014', 'Albania_4015', 'Albania_4016', 'Albania_4017', 'Algeria_4001', 'Algeria_4002', 'Algeria_4003', 'Algeria_4004', 'Algeria_4005', 'Algeria_4006', 'Algeria_4007', 'Algeria_4008', 'Algeria_4009', 'Algeria_4010', 'Algeria_4011', 'Algeria_4012', 'Algeria_4013', 'Algeria_4014', 'Algeria_4015', 'Algeria_4016', 'Algeria_4017', 'Angola_4001', 'Angola_4002', 'Angola_4003', 'Angola_4004', 'Angola_4005', 'Angola_4006', 'Angola_4007', 'Angola_4008', 'Angola_4009', 'Angola_4010', 'Angola_4011', 'Angola_4012', 'Angola_4013', 'Angola_4014', 'Angola_4015', 'Angola_4016', 'Angola_4017', 'Antigua and Barbuda_4001', 'Antigua and Barbuda_4002', 'Antigua and Barbuda_4003', 'Antigua and Barbuda_4004', 'Antigua and Barbuda_4005', 'Antigua and Barbuda_4006', 'Antigua and Barbuda_4007', 'Antigua and Barbuda_4008', 'Antigua and Barbuda_4009', 'Antigua and Barbuda_4010', 'Antigua and Barbuda_4011', 'Antigua and Barbuda_4012', 'Antigua and Barbuda_4013', 'Antigua and Barbuda_4014', 'Antigua and Barbuda_4015', 'Antigua and Barbuda_4016', 'Antigua and Barbuda_4017', 'Argentina_4001', 'Argentina_4002', 'Argentina_4003', 'Argentina_4004', 'Argentina_4005', 'Argentina_4006', 'Argentina_4007', 'Argentina_4008', 'Argentina_4009', 'Argentina_4010', 'Argentina_4011', 'Argentina_4012', 'Argentina_4013', 'Argentina_4014', 'Argentina_4015', 'Argentina_4016', 'Argentina_4017', 'Armenia_4001', 'Armenia_4002', 'Armenia_4003', 'Armenia_4004', 'Armenia_4005', 'Armenia_4006', 'Armenia_4007', 'Armenia_4008', 'Armenia_4009', 'Armenia_4010', 'Armenia_4011', 'Armenia_4012', 'Armenia_4013', 'Armenia_4014', 'Armenia_4015', 'Armenia_4016', 'Armenia_4017', 'Australia_4001', 'Australia_4002', 'Australia_4003', 'Australia_4004', 'Australia_4005', 'Australia_4006', 'Australia_4007', 'Australia_4008', 'Australia_4009', 'Australia_4010', 'Australia_4011', 'Australia_4012', 'Australia_4013', 'Australia_4014', 'Australia_4015', 'Australia_4016', 'Australia_4017', 'Austria_4001', 'Austria_4002', 'Austria_4003', 'Austria_4004', 'Austria_4005', 'Austria_4006', 'Austria_4007', 'Austria_4008', 'Austria_4009', 'Austria_4010', 'Austria_4011', 'Austria_4012', 'Austria_4013', 'Austria_4014', 'Austria_4015', 'Austria_4016', 'Austria_4017', 'Azerbaijan_4001', 'Azerbaijan_4002', 'Azerbaijan_4003', 'Azerbaijan_4004', 'Azerbaijan_4005', 'Azerbaijan_4006', 'Azerbaijan_4007', 'Azerbaijan_4008', 'Azerbaijan_4009', 'Azerbaijan_4010', 'Azerbaijan_4011', 'Azerbaijan_4012', 'Azerbaijan_4013', 'Azerbaijan_4014', 'Azerbaijan_4015', 'Azerbaijan_4016', 'Azerbaijan_4017', 'Bahrain_4001', 'Bahrain_4002', 'Bahrain_4003', 'Bahrain_4004', 'Bahrain_4005', 'Bahrain_4006', 'Bahrain_4007', 'Bahrain_4008', 'Bahrain_4009', 'Bahrain_4010', 'Bahrain_4011', 'Bahrain_4012', 'Bahrain_4013', 'Bahrain_4014', 'Bahrain_4015', 'Bahrain_4016', 'Bahrain_4017', 'Barbados_4001', 'Barbados_4002', 'Barbados_4003', 'Barbados_4004', 'Barbados_4005', 'Barbados_4006', 'Barbados_4007', 'Barbados_4008', 'Barbados_4009', 'Barbados_4010', 'Barbados_4011', 'Barbados_4012', 'Barbados_4013', 'Barbados_4014', 'Barbados_4015', 'Barbados_4016', 'Barbados_4017', 'Belgium_4001', 'Belgium_4002', 'Belgium_4003', 'Belgium_4004', 'Belgium_4005', 'Belgium_4006', 'Belgium_4007', 'Belgium_4008', 'Belgium_4009', 'Belgium_4010', 'Belgium_4011', 'Belgium_4012', 'Belgium_4013', 'Belgium_4014', 'Belgium_4015', 'Belgium_4016', 'Belgium_4017', 'Belize_4001', 'Belize_4002', 'Belize_4003', 'Belize_4004', 'Belize_4005', 'Belize_4006', 'Belize_4007', 'Belize_4008', 'Belize_4009', 'Belize_4010', 'Belize_4011', 'Belize_4012', 'Belize_4013', 'Belize_4014', 'Belize_4015', 'Belize_4016', 'Belize_4017', 'Benin_4001', 'Benin_4002', 'Benin_4003', 'Benin_4004', 'Benin_4005', 'Benin_4006', 'Benin_4007', 'Benin_4008', 'Benin_4009', 'Benin_4010', 'Benin_4011', 'Benin_4012', 'Benin_4013', 'Benin_4014', 'Benin_4015', 'Benin_4016', 'Benin_4017', 'Bermuda_4001', 'Bermuda_4002', 'Bermuda_4003', 'Bermuda_4004', 'Bermuda_4005', 'Bermuda_4006', 'Bermuda_4007', 'Bermuda_4008', 'Bermuda_4009', 'Bermuda_4010', 'Bermuda_4011', 'Bermuda_4012', 'Bermuda_4013', 'Bermuda_4014', 'Bermuda_4015', 'Bermuda_4016', 'Bermuda_4017', 'Bolivia, Plurinational State of_4001', 'Bolivia, Plurinational State of_4002', 'Bolivia, Plurinational State of_4003', 'Bolivia, Plurinational State of_4004', 'Bolivia, Plurinational State of_4005', 'Bolivia, Plurinational State of_4006', 'Bolivia, Plurinational State of_4007', 'Bolivia, Plurinational State of_4008', 'Bolivia, Plurinational State of_4009', 'Bolivia, Plurinational State of_4010', 'Bolivia, Plurinational State of_4011', 'Bolivia, Plurinational State of_4012', 'Bolivia, Plurinational State of_4013', 'Bolivia, Plurinational State of_4014', 'Bolivia, Plurinational State of_4015', 'Bolivia, Plurinational State of_4016', 'Bolivia, Plurinational State of_4017', 'Bosnia and Herzegovina_4001', 'Bosnia and Herzegovina_4002', 'Bosnia and Herzegovina_4003', 'Bosnia and Herzegovina_4004', 'Bosnia and Herzegovina_4005', 'Bosnia and Herzegovina_4006', 'Bosnia and Herzegovina_4007', 'Bosnia and Herzegovina_4008', 'Bosnia and Herzegovina_4009', 'Bosnia and Herzegovina_4010', 'Bosnia and Herzegovina_4011', 'Bosnia and Herzegovina_4012', 'Bosnia and Herzegovina_4013', 'Bosnia and Herzegovina_4014', 'Bosnia and Herzegovina_4015', 'Bosnia and Herzegovina_4016', 'Bosnia and Herzegovina_4017', 'Botswana_4001', 'Botswana_4002', 'Botswana_4003', 'Botswana_4004', 'Botswana_4005', 'Botswana_4006', 'Botswana_4007', 'Botswana_4008', 'Botswana_4009', 'Botswana_4010', 'Botswana_4011', 'Botswana_4012', 'Botswana_4013', 'Botswana_4014', 'Botswana_4015', 'Botswana_4016', 'Botswana_4017', 'Brazil_4001', 'Brazil_4002', 'Brazil_4003', 'Brazil_4004', 'Brazil_4005', 'Brazil_4006', 'Brazil_4007', 'Brazil_4008', 'Brazil_4009', 'Brazil_4010', 'Brazil_4011', 'Brazil_4012', 'Brazil_4013', 'Brazil_4014', 'Brazil_4015', 'Brazil_4016', 'Brazil_4017', 'Brunei Darussalam_4001', 'Brunei Darussalam_4002', 'Brunei Darussalam_4003', 'Brunei Darussalam_4004', 'Brunei Darussalam_4005', 'Brunei Darussalam_4006', 'Brunei Darussalam_4007', 'Brunei Darussalam_4008', 'Brunei Darussalam_4009', 'Brunei Darussalam_4010', 'Brunei Darussalam_4011', 'Brunei Darussalam_4012', 'Brunei Darussalam_4013', 'Brunei Darussalam_4014', 'Brunei Darussalam_4015', 'Brunei Darussalam_4016', 'Brunei Darussalam_4017', 'Bulgaria_4001', 'Bulgaria_4002', 'Bulgaria_4003', 'Bulgaria_4004', 'Bulgaria_4005', 'Bulgaria_4006', 'Bulgaria_4007', 'Bulgaria_4008', 'Bulgaria_4009', 'Bulgaria_4010', 'Bulgaria_4011', 'Bulgaria_4012', 'Bulgaria_4013', 'Bulgaria_4014', 'Bulgaria_4015', 'Bulgaria_4016', 'Bulgaria_4017', 'Burkina Faso_4001', 'Burkina Faso_4002', 'Burkina Faso_4003', 'Burkina Faso_4004', 'Burkina Faso_4005', 'Burkina Faso_4006', 'Burkina Faso_4007', 'Burkina Faso_4008', 'Burkina Faso_4009', 'Burkina Faso_4010', 'Burkina Faso_4011', 'Burkina Faso_4012', 'Burkina Faso_4013', 'Burkina Faso_4014', 'Burkina Faso_4015', 'Burkina Faso_4016', 'Burkina Faso_4017', 'Burundi_4001', 'Burundi_4002', 'Burundi_4003', 'Burundi_4004', 'Burundi_4005', 'Burundi_4006', 'Burundi_4007', 'Burundi_4008', 'Burundi_4009', 'Burundi_4010', 'Burundi_4011', 'Burundi_4012', 'Burundi_4013', 'Burundi_4014', 'Burundi_4015', 'Burundi_4016', 'Burundi_4017', 'Cambodia_4001', 'Cambodia_4002', 'Cambodia_4003', 'Cambodia_4004', 'Cambodia_4005', 'Cambodia_4006', 'Cambodia_4007', 'Cambodia_4008', 'Cambodia_4009', 'Cambodia_4010', 'Cambodia_4011', 'Cambodia_4012', 'Cambodia_4013', 'Cambodia_4014', 'Cambodia_4015', 'Cambodia_4016', 'Cambodia_4017', 'Cameroon_4001', 'Cameroon_4002', 'Cameroon_4003', 'Cameroon_4004', 'Cameroon_4005', 'Cameroon_4006', 'Cameroon_4007', 'Cameroon_4008', 'Cameroon_4009', 'Cameroon_4010', 'Cameroon_4011', 'Cameroon_4012', 'Cameroon_4013', 'Cameroon_4014', 'Cameroon_4015', 'Cameroon_4016', 'Cameroon_4017', 'Canada_4001', 'Canada_4002', 'Canada_4003', 'Canada_4004', 'Canada_4005', 'Canada_4006', 'Canada_4007', 'Canada_4008', 'Canada_4009', 'Canada_4010', 'Canada_4011', 'Canada_4012', 'Canada_4013', 'Canada_4014', 'Canada_4015', 'Canada_4016', 'Canada_4017', 'Central African Republic_4001', 'Central African Republic_4002', 'Central African Republic_4003', 'Central African Republic_4004', 'Central African Republic_4005', 'Central African Republic_4006', 'Central African Republic_4007', 'Central African Republic_4008', 'Central African Republic_4009', 'Central African Republic_4010', 'Central African Republic_4011', 'Central African Republic_4012', 'Central African Republic_4013', 'Central African Republic_4014', 'Central African Republic_4015', 'Central African Republic_4016', 'Central African Republic_4017', 'Chile_4001', 'Chile_4002', 'Chile_4003', 'Chile_4004', 'Chile_4005', 'Chile_4006', 'Chile_4007', 'Chile_4008', 'Chile_4009', 'Chile_4010', 'Chile_4011', 'Chile_4012', 'Chile_4013', 'Chile_4014', 'Chile_4015', 'Chile_4016', 'Chile_4017', 'China_4001', 'China_4002', 'China_4003', 'China_4004', 'China_4005', 'China_4006', 'China_4007', 'China_4008', 'China_4009', 'China_4010', 'China_4011', 'China_4012', 'China_4013', 'China_4014', 'China_4015', 'China_4016', 'China_4017', 'Colombia_4001', 'Colombia_4002', 'Colombia_4003', 'Colombia_4004', 'Colombia_4005', 'Colombia_4006', 'Colombia_4007', 'Colombia_4008', 'Colombia_4009', 'Colombia_4010', 'Colombia_4011', 'Colombia_4012', 'Colombia_4013', 'Colombia_4014', 'Colombia_4015', 'Colombia_4016', 'Colombia_4017', 'Congo_4001', 'Congo_4002', 'Congo_4003', 'Congo_4004', 'Congo_4005', 'Congo_4006', 'Congo_4007', 'Congo_4008', 'Congo_4009', 'Congo_4010', 'Congo_4011', 'Congo_4012', 'Congo_4013', 'Congo_4014', 'Congo_4015', 'Congo_4016', 'Congo_4017', 'Costa Rica_4001', 'Costa Rica_4002', 'Costa Rica_4003', 'Costa Rica_4004', 'Costa Rica_4005', 'Costa Rica_4006', 'Costa Rica_4007', 'Costa Rica_4008', 'Costa Rica_4009', 'Costa Rica_4010', 'Costa Rica_4011', 'Costa Rica_4012', 'Costa Rica_4013', 'Costa Rica_4014', 'Costa Rica_4015', 'Costa Rica_4016', 'Costa Rica_4017', 'Croatia_4001', 'Croatia_4002', 'Croatia_4003', 'Croatia_4004', 'Croatia_4005', 'Croatia_4006', 'Croatia_4007', 'Croatia_4008', 'Croatia_4009', 'Croatia_4010', 'Croatia_4011', 'Croatia_4012', 'Croatia_4013', 'Croatia_4014', 'Croatia_4015', 'Croatia_4016', 'Croatia_4017', 'Cyprus_4001', 'Cyprus_4002', 'Cyprus_4003', 'Cyprus_4004', 'Cyprus_4005', 'Cyprus_4006', 'Cyprus_4007', 'Cyprus_4008', 'Cyprus_4009', 'Cyprus_4010', 'Cyprus_4011', 'Cyprus_4012', 'Cyprus_4013', 'Cyprus_4014', 'Cyprus_4015', 'Cyprus_4016', 'Cyprus_4017', 'Czech Republic_4001', 'Czech Republic_4002', 'Czech Republic_4003', 'Czech Republic_4004', 'Czech Republic_4005', 'Czech Republic_4006', 'Czech Republic_4007', 'Czech Republic_4008', 'Czech Republic_4009', 'Czech Republic_4010', 'Czech Republic_4011', 'Czech Republic_4012', 'Czech Republic_4013', 'Czech Republic_4014', 'Czech Republic_4015', 'Czech Republic_4016', 'Czech Republic_4017', \"Côte d'Ivoire_4001\", \"Côte d'Ivoire_4002\", \"Côte d'Ivoire_4003\", \"Côte d'Ivoire_4004\", \"Côte d'Ivoire_4005\", \"Côte d'Ivoire_4006\", \"Côte d'Ivoire_4007\", \"Côte d'Ivoire_4008\", \"Côte d'Ivoire_4009\", \"Côte d'Ivoire_4010\", \"Côte d'Ivoire_4011\", \"Côte d'Ivoire_4012\", \"Côte d'Ivoire_4013\", \"Côte d'Ivoire_4014\", \"Côte d'Ivoire_4015\", \"Côte d'Ivoire_4016\", \"Côte d'Ivoire_4017\", 'Denmark_4001', 'Denmark_4002', 'Denmark_4003', 'Denmark_4004', 'Denmark_4005', 'Denmark_4006', 'Denmark_4007', 'Denmark_4008', 'Denmark_4009', 'Denmark_4010', 'Denmark_4011', 'Denmark_4012', 'Denmark_4013', 'Denmark_4014', 'Denmark_4015', 'Denmark_4016', 'Denmark_4017', 'Dominica_4001', 'Dominica_4002', 'Dominica_4003', 'Dominica_4004', 'Dominica_4005', 'Dominica_4006', 'Dominica_4007', 'Dominica_4008', 'Dominica_4009', 'Dominica_4010', 'Dominica_4011', 'Dominica_4012', 'Dominica_4013', 'Dominica_4014', 'Dominica_4015', 'Dominica_4016', 'Dominica_4017', 'Dominican Republic_4001', 'Dominican Republic_4002', 'Dominican Republic_4003', 'Dominican Republic_4004', 'Dominican Republic_4005', 'Dominican Republic_4006', 'Dominican Republic_4007', 'Dominican Republic_4008', 'Dominican Republic_4009', 'Dominican Republic_4010', 'Dominican Republic_4011', 'Dominican Republic_4012', 'Dominican Republic_4013', 'Dominican Republic_4014', 'Dominican Republic_4015', 'Dominican Republic_4016', 'Dominican Republic_4017', 'Ecuador_4001', 'Ecuador_4002', 'Ecuador_4003', 'Ecuador_4004', 'Ecuador_4005', 'Ecuador_4006', 'Ecuador_4007', 'Ecuador_4008', 'Ecuador_4009', 'Ecuador_4010', 'Ecuador_4011', 'Ecuador_4012', 'Ecuador_4013', 'Ecuador_4014', 'Ecuador_4015', 'Ecuador_4016', 'Ecuador_4017', 'Egypt_4001', 'Egypt_4002', 'Egypt_4003', 'Egypt_4004', 'Egypt_4005', 'Egypt_4006', 'Egypt_4007', 'Egypt_4008', 'Egypt_4009', 'Egypt_4010', 'Egypt_4011', 'Egypt_4012', 'Egypt_4013', 'Egypt_4014', 'Egypt_4015', 'Egypt_4016', 'Egypt_4017', 'El Salvador_4001', 'El Salvador_4002', 'El Salvador_4003', 'El Salvador_4004', 'El Salvador_4005', 'El Salvador_4006', 'El Salvador_4007', 'El Salvador_4008', 'El Salvador_4009', 'El Salvador_4010', 'El Salvador_4011', 'El Salvador_4012', 'El Salvador_4013', 'El Salvador_4014', 'El Salvador_4015', 'El Salvador_4016', 'El Salvador_4017', 'Estonia_4001', 'Estonia_4002', 'Estonia_4003', 'Estonia_4004', 'Estonia_4005', 'Estonia_4006', 'Estonia_4007', 'Estonia_4008', 'Estonia_4009', 'Estonia_4010', 'Estonia_4011', 'Estonia_4012', 'Estonia_4013', 'Estonia_4014', 'Estonia_4015', 'Estonia_4016', 'Estonia_4017', 'Eswatini_4001', 'Eswatini_4002', 'Eswatini_4003', 'Eswatini_4004', 'Eswatini_4005', 'Eswatini_4006', 'Eswatini_4007', 'Eswatini_4008', 'Eswatini_4009', 'Eswatini_4010', 'Eswatini_4011', 'Eswatini_4012', 'Eswatini_4013', 'Eswatini_4014', 'Eswatini_4015', 'Eswatini_4016', 'Eswatini_4017', 'Ethiopia_4001', 'Ethiopia_4002', 'Ethiopia_4003', 'Ethiopia_4004', 'Ethiopia_4005', 'Ethiopia_4006', 'Ethiopia_4007', 'Ethiopia_4008', 'Ethiopia_4009', 'Ethiopia_4010', 'Ethiopia_4011', 'Ethiopia_4012', 'Ethiopia_4013', 'Ethiopia_4014', 'Ethiopia_4015', 'Ethiopia_4016', 'Ethiopia_4017', 'Fiji_4001', 'Fiji_4002', 'Fiji_4003', 'Fiji_4004', 'Fiji_4005', 'Fiji_4006', 'Fiji_4007', 'Fiji_4008', 'Fiji_4009', 'Fiji_4010', 'Fiji_4011', 'Fiji_4012', 'Fiji_4013', 'Fiji_4014', 'Fiji_4015', 'Fiji_4016', 'Fiji_4017', 'Finland_4001', 'Finland_4002', 'Finland_4003', 'Finland_4004', 'Finland_4005', 'Finland_4006', 'Finland_4007', 'Finland_4008', 'Finland_4009', 'Finland_4010', 'Finland_4011', 'Finland_4012', 'Finland_4013', 'Finland_4014', 'Finland_4015', 'Finland_4016', 'Finland_4017', 'France_4001', 'France_4002', 'France_4003', 'France_4004', 'France_4005', 'France_4006', 'France_4007', 'France_4008', 'France_4009', 'France_4010', 'France_4011', 'France_4012', 'France_4013', 'France_4014', 'France_4015', 'France_4016', 'France_4017', 'French Polynesia_4001', 'French Polynesia_4002', 'French Polynesia_4003', 'French Polynesia_4004', 'French Polynesia_4005', 'French Polynesia_4006', 'French Polynesia_4007', 'French Polynesia_4008', 'French Polynesia_4009', 'French Polynesia_4010', 'French Polynesia_4011', 'French Polynesia_4012', 'French Polynesia_4013', 'French Polynesia_4014', 'French Polynesia_4015', 'French Polynesia_4016', 'French Polynesia_4017', 'Gambia_4001', 'Gambia_4002', 'Gambia_4003', 'Gambia_4004', 'Gambia_4005', 'Gambia_4006', 'Gambia_4007', 'Gambia_4008', 'Gambia_4009', 'Gambia_4010', 'Gambia_4011', 'Gambia_4012', 'Gambia_4013', 'Gambia_4014', 'Gambia_4015', 'Gambia_4016', 'Gambia_4017', 'Georgia_4001', 'Georgia_4002', 'Georgia_4003', 'Georgia_4004', 'Georgia_4005', 'Georgia_4006', 'Georgia_4007', 'Georgia_4008', 'Georgia_4009', 'Georgia_4010', 'Georgia_4011', 'Georgia_4012', 'Georgia_4013', 'Georgia_4014', 'Georgia_4015', 'Georgia_4016', 'Georgia_4017', 'Germany_4001', 'Germany_4002', 'Germany_4003', 'Germany_4004', 'Germany_4005', 'Germany_4006', 'Germany_4007', 'Germany_4008', 'Germany_4009', 'Germany_4010', 'Germany_4011', 'Germany_4012', 'Germany_4013', 'Germany_4014', 'Germany_4015', 'Germany_4016', 'Germany_4017', 'Ghana_4001', 'Ghana_4002', 'Ghana_4003', 'Ghana_4004', 'Ghana_4005', 'Ghana_4006', 'Ghana_4007', 'Ghana_4008', 'Ghana_4009', 'Ghana_4010', 'Ghana_4011', 'Ghana_4012', 'Ghana_4013', 'Ghana_4014', 'Ghana_4015', 'Ghana_4016', 'Ghana_4017', 'Greece_4001', 'Greece_4002', 'Greece_4003', 'Greece_4004', 'Greece_4005', 'Greece_4006', 'Greece_4007', 'Greece_4008', 'Greece_4009', 'Greece_4010', 'Greece_4011', 'Greece_4012', 'Greece_4013', 'Greece_4014', 'Greece_4015', 'Greece_4016', 'Greece_4017', 'Guatemala_4001', 'Guatemala_4002', 'Guatemala_4003', 'Guatemala_4004', 'Guatemala_4005', 'Guatemala_4006', 'Guatemala_4007', 'Guatemala_4008', 'Guatemala_4009', 'Guatemala_4010', 'Guatemala_4011', 'Guatemala_4012', 'Guatemala_4013', 'Guatemala_4014', 'Guatemala_4015', 'Guatemala_4016', 'Guatemala_4017', 'Guinea_4001', 'Guinea_4002', 'Guinea_4003', 'Guinea_4004', 'Guinea_4005', 'Guinea_4006', 'Guinea_4007', 'Guinea_4008', 'Guinea_4009', 'Guinea_4010', 'Guinea_4011', 'Guinea_4012', 'Guinea_4013', 'Guinea_4014', 'Guinea_4015', 'Guinea_4016', 'Guinea_4017', 'Guyana_4001', 'Guyana_4002', 'Guyana_4003', 'Guyana_4004', 'Guyana_4005', 'Guyana_4006', 'Guyana_4007', 'Guyana_4008', 'Guyana_4009', 'Guyana_4010', 'Guyana_4011', 'Guyana_4012', 'Guyana_4013', 'Guyana_4014', 'Guyana_4015', 'Guyana_4016', 'Guyana_4017', 'Haiti_4001', 'Haiti_4002', 'Haiti_4003', 'Haiti_4004', 'Haiti_4005', 'Haiti_4006', 'Haiti_4007', 'Haiti_4008', 'Haiti_4009', 'Haiti_4010', 'Haiti_4011', 'Haiti_4012', 'Haiti_4013', 'Haiti_4014', 'Haiti_4015', 'Haiti_4016', 'Haiti_4017', 'Honduras_4001', 'Honduras_4002', 'Honduras_4003', 'Honduras_4004', 'Honduras_4005', 'Honduras_4006', 'Honduras_4007', 'Honduras_4008', 'Honduras_4009', 'Honduras_4010', 'Honduras_4011', 'Honduras_4012', 'Honduras_4013', 'Honduras_4014', 'Honduras_4015', 'Honduras_4016', 'Honduras_4017', 'Hungary_4001', 'Hungary_4002', 'Hungary_4003', 'Hungary_4004', 'Hungary_4005', 'Hungary_4006', 'Hungary_4007', 'Hungary_4008', 'Hungary_4009', 'Hungary_4010', 'Hungary_4011', 'Hungary_4012', 'Hungary_4013', 'Hungary_4014', 'Hungary_4015', 'Hungary_4016', 'Hungary_4017', 'Iceland_4001', 'Iceland_4002', 'Iceland_4003', 'Iceland_4004', 'Iceland_4005', 'Iceland_4006', 'Iceland_4007', 'Iceland_4008', 'Iceland_4009', 'Iceland_4010', 'Iceland_4011', 'Iceland_4012', 'Iceland_4013', 'Iceland_4014', 'Iceland_4015', 'Iceland_4016', 'Iceland_4017', 'India_4001', 'India_4002', 'India_4003', 'India_4004', 'India_4005', 'India_4006', 'India_4007', 'India_4008', 'India_4009', 'India_4010', 'India_4011', 'India_4012', 'India_4013', 'India_4014', 'India_4015', 'India_4016', 'India_4017', 'Indonesia_4001', 'Indonesia_4002', 'Indonesia_4003', 'Indonesia_4004', 'Indonesia_4005', 'Indonesia_4006', 'Indonesia_4007', 'Indonesia_4008', 'Indonesia_4009', 'Indonesia_4010', 'Indonesia_4011', 'Indonesia_4012', 'Indonesia_4013', 'Indonesia_4014', 'Indonesia_4015', 'Indonesia_4016', 'Indonesia_4017', 'Ireland_4001', 'Ireland_4002', 'Ireland_4003', 'Ireland_4004', 'Ireland_4005', 'Ireland_4006', 'Ireland_4007', 'Ireland_4008', 'Ireland_4009', 'Ireland_4010', 'Ireland_4011', 'Ireland_4012', 'Ireland_4013', 'Ireland_4014', 'Ireland_4015', 'Ireland_4016', 'Ireland_4017', 'Israel_4001', 'Israel_4002', 'Israel_4003', 'Israel_4004', 'Israel_4005', 'Israel_4006', 'Israel_4007', 'Israel_4008', 'Israel_4009', 'Israel_4010', 'Israel_4011', 'Israel_4012', 'Israel_4013', 'Israel_4014', 'Israel_4015', 'Israel_4016', 'Israel_4017', 'Italy_4001', 'Italy_4002', 'Italy_4003', 'Italy_4004', 'Italy_4005', 'Italy_4006', 'Italy_4007', 'Italy_4008', 'Italy_4009', 'Italy_4010', 'Italy_4011', 'Italy_4012', 'Italy_4013', 'Italy_4014', 'Italy_4015', 'Italy_4016', 'Italy_4017', 'Jamaica_4001', 'Jamaica_4002', 'Jamaica_4003', 'Jamaica_4004', 'Jamaica_4005', 'Jamaica_4006', 'Jamaica_4007', 'Jamaica_4008', 'Jamaica_4009', 'Jamaica_4010', 'Jamaica_4011', 'Jamaica_4012', 'Jamaica_4013', 'Jamaica_4014', 'Jamaica_4015', 'Jamaica_4016', 'Jamaica_4017', 'Japan_4001', 'Japan_4002', 'Japan_4003', 'Japan_4004', 'Japan_4005', 'Japan_4006', 'Japan_4007', 'Japan_4008', 'Japan_4009', 'Japan_4010', 'Japan_4011', 'Japan_4012', 'Japan_4013', 'Japan_4014', 'Japan_4015', 'Japan_4016', 'Japan_4017', 'Jordan_4001', 'Jordan_4002', 'Jordan_4003', 'Jordan_4004', 'Jordan_4005', 'Jordan_4006', 'Jordan_4007', 'Jordan_4008', 'Jordan_4009', 'Jordan_4010', 'Jordan_4011', 'Jordan_4012', 'Jordan_4013', 'Jordan_4014', 'Jordan_4015', 'Jordan_4016', 'Jordan_4017', 'Kazakhstan_4001', 'Kazakhstan_4002', 'Kazakhstan_4003', 'Kazakhstan_4004', 'Kazakhstan_4005', 'Kazakhstan_4006', 'Kazakhstan_4007', 'Kazakhstan_4008', 'Kazakhstan_4009', 'Kazakhstan_4010', 'Kazakhstan_4011', 'Kazakhstan_4012', 'Kazakhstan_4013', 'Kazakhstan_4014', 'Kazakhstan_4015', 'Kazakhstan_4016', 'Kazakhstan_4017', 'Kenya_4001', 'Kenya_4002', 'Kenya_4003', 'Kenya_4004', 'Kenya_4005', 'Kenya_4006', 'Kenya_4007', 'Kenya_4008', 'Kenya_4009', 'Kenya_4010', 'Kenya_4011', 'Kenya_4012', 'Kenya_4013', 'Kenya_4014', 'Kenya_4015', 'Kenya_4016', 'Kenya_4017', 'Kiribati_4001', 'Kiribati_4002', 'Kiribati_4003', 'Kiribati_4004', 'Kiribati_4005', 'Kiribati_4006', 'Kiribati_4007', 'Kiribati_4008', 'Kiribati_4009', 'Kiribati_4010', 'Kiribati_4011', 'Kiribati_4012', 'Kiribati_4013', 'Kiribati_4014', 'Kiribati_4015', 'Kiribati_4016', 'Kiribati_4017', 'Korea, Republic of_4001', 'Korea, Republic of_4002', 'Korea, Republic of_4003', 'Korea, Republic of_4004', 'Korea, Republic of_4005', 'Korea, Republic of_4006', 'Korea, Republic of_4007', 'Korea, Republic of_4008', 'Korea, Republic of_4009', 'Korea, Republic of_4010', 'Korea, Republic of_4011', 'Korea, Republic of_4012', 'Korea, Republic of_4013', 'Korea, Republic of_4014', 'Korea, Republic of_4015', 'Korea, Republic of_4016', 'Korea, Republic of_4017', 'Kuwait_4001', 'Kuwait_4002', 'Kuwait_4003', 'Kuwait_4004', 'Kuwait_4005', 'Kuwait_4006', 'Kuwait_4007', 'Kuwait_4008', 'Kuwait_4009', 'Kuwait_4010', 'Kuwait_4011', 'Kuwait_4012', 'Kuwait_4013', 'Kuwait_4014', 'Kuwait_4015', 'Kuwait_4016', 'Kuwait_4017', 'Kyrgyzstan_4001', 'Kyrgyzstan_4002', 'Kyrgyzstan_4003', 'Kyrgyzstan_4004', 'Kyrgyzstan_4005', 'Kyrgyzstan_4006', 'Kyrgyzstan_4007', 'Kyrgyzstan_4008', 'Kyrgyzstan_4009', 'Kyrgyzstan_4010', 'Kyrgyzstan_4011', 'Kyrgyzstan_4012', 'Kyrgyzstan_4013', 'Kyrgyzstan_4014', 'Kyrgyzstan_4015', 'Kyrgyzstan_4016', 'Kyrgyzstan_4017', 'Latvia_4001', 'Latvia_4002', 'Latvia_4003', 'Latvia_4004', 'Latvia_4005', 'Latvia_4006', 'Latvia_4007', 'Latvia_4008', 'Latvia_4009', 'Latvia_4010', 'Latvia_4011', 'Latvia_4012', 'Latvia_4013', 'Latvia_4014', 'Latvia_4015', 'Latvia_4016', 'Latvia_4017', 'Lebanon_4001', 'Lebanon_4002', 'Lebanon_4003', 'Lebanon_4004', 'Lebanon_4005', 'Lebanon_4006', 'Lebanon_4007', 'Lebanon_4008', 'Lebanon_4009', 'Lebanon_4010', 'Lebanon_4011', 'Lebanon_4012', 'Lebanon_4013', 'Lebanon_4014', 'Lebanon_4015', 'Lebanon_4016', 'Lebanon_4017', 'Lithuania_4001', 'Lithuania_4002', 'Lithuania_4003', 'Lithuania_4004', 'Lithuania_4005', 'Lithuania_4006', 'Lithuania_4007', 'Lithuania_4008', 'Lithuania_4009', 'Lithuania_4010', 'Lithuania_4011', 'Lithuania_4012', 'Lithuania_4013', 'Lithuania_4014', 'Lithuania_4015', 'Lithuania_4016', 'Lithuania_4017', 'Luxembourg_4001', 'Luxembourg_4002', 'Luxembourg_4003', 'Luxembourg_4004', 'Luxembourg_4005', 'Luxembourg_4006', 'Luxembourg_4007', 'Luxembourg_4008', 'Luxembourg_4009', 'Luxembourg_4010', 'Luxembourg_4011', 'Luxembourg_4012', 'Luxembourg_4013', 'Luxembourg_4014', 'Luxembourg_4015', 'Luxembourg_4016', 'Luxembourg_4017', 'Madagascar_4001', 'Madagascar_4002', 'Madagascar_4003', 'Madagascar_4004', 'Madagascar_4005', 'Madagascar_4006', 'Madagascar_4007', 'Madagascar_4008', 'Madagascar_4009', 'Madagascar_4010', 'Madagascar_4011', 'Madagascar_4012', 'Madagascar_4013', 'Madagascar_4014', 'Madagascar_4015', 'Madagascar_4016', 'Madagascar_4017', 'Malawi_4001', 'Malawi_4002', 'Malawi_4003', 'Malawi_4004', 'Malawi_4005', 'Malawi_4006', 'Malawi_4007', 'Malawi_4008', 'Malawi_4009', 'Malawi_4010', 'Malawi_4011', 'Malawi_4012', 'Malawi_4013', 'Malawi_4014', 'Malawi_4015', 'Malawi_4016', 'Malawi_4017', 'Malaysia_4001', 'Malaysia_4002', 'Malaysia_4003', 'Malaysia_4004', 'Malaysia_4005', 'Malaysia_4006', 'Malaysia_4007', 'Malaysia_4008', 'Malaysia_4009', 'Malaysia_4010', 'Malaysia_4011', 'Malaysia_4012', 'Malaysia_4013', 'Malaysia_4014', 'Malaysia_4015', 'Malaysia_4016', 'Malaysia_4017', 'Maldives_4001', 'Maldives_4002', 'Maldives_4003', 'Maldives_4004', 'Maldives_4005', 'Maldives_4006', 'Maldives_4007', 'Maldives_4008', 'Maldives_4009', 'Maldives_4010', 'Maldives_4011', 'Maldives_4012', 'Maldives_4013', 'Maldives_4014', 'Maldives_4015', 'Maldives_4016', 'Maldives_4017', 'Malta_4001', 'Malta_4002', 'Malta_4003', 'Malta_4004', 'Malta_4005', 'Malta_4006', 'Malta_4007', 'Malta_4008', 'Malta_4009', 'Malta_4010', 'Malta_4011', 'Malta_4012', 'Malta_4013', 'Malta_4014', 'Malta_4015', 'Malta_4016', 'Malta_4017', 'Mauritius_4001', 'Mauritius_4002', 'Mauritius_4003', 'Mauritius_4004', 'Mauritius_4005', 'Mauritius_4006', 'Mauritius_4007', 'Mauritius_4008', 'Mauritius_4009', 'Mauritius_4010', 'Mauritius_4011', 'Mauritius_4012', 'Mauritius_4013', 'Mauritius_4014', 'Mauritius_4015', 'Mauritius_4016', 'Mauritius_4017', 'Mayotte_4001', 'Mayotte_4002', 'Mayotte_4003', 'Mayotte_4004', 'Mayotte_4005', 'Mayotte_4006', 'Mayotte_4007', 'Mayotte_4008', 'Mayotte_4009', 'Mayotte_4010', 'Mayotte_4011', 'Mayotte_4012', 'Mayotte_4013', 'Mayotte_4014', 'Mayotte_4015', 'Mayotte_4016', 'Mayotte_4017', 'Mexico_4001', 'Mexico_4002', 'Mexico_4003', 'Mexico_4004', 'Mexico_4005', 'Mexico_4006', 'Mexico_4007', 'Mexico_4008', 'Mexico_4009', 'Mexico_4010', 'Mexico_4011', 'Mexico_4012', 'Mexico_4013', 'Mexico_4014', 'Mexico_4015', 'Mexico_4016', 'Mexico_4017', 'Mongolia_4001', 'Mongolia_4002', 'Mongolia_4003', 'Mongolia_4004', 'Mongolia_4005', 'Mongolia_4006', 'Mongolia_4007', 'Mongolia_4008', 'Mongolia_4009', 'Mongolia_4010', 'Mongolia_4011', 'Mongolia_4012', 'Mongolia_4013', 'Mongolia_4014', 'Mongolia_4015', 'Mongolia_4016', 'Mongolia_4017', 'Morocco_4001', 'Morocco_4002', 'Morocco_4003', 'Morocco_4004', 'Morocco_4005', 'Morocco_4006', 'Morocco_4007', 'Morocco_4008', 'Morocco_4009', 'Morocco_4010', 'Morocco_4011', 'Morocco_4012', 'Morocco_4013', 'Morocco_4014', 'Morocco_4015', 'Morocco_4016', 'Morocco_4017', 'Mozambique_4001', 'Mozambique_4002', 'Mozambique_4003', 'Mozambique_4004', 'Mozambique_4005', 'Mozambique_4006', 'Mozambique_4007', 'Mozambique_4008', 'Mozambique_4009', 'Mozambique_4010', 'Mozambique_4011', 'Mozambique_4012', 'Mozambique_4013', 'Mozambique_4014', 'Mozambique_4015', 'Mozambique_4016', 'Mozambique_4017', 'Namibia_4001', 'Namibia_4002', 'Namibia_4003', 'Namibia_4004', 'Namibia_4005', 'Namibia_4006', 'Namibia_4007', 'Namibia_4008', 'Namibia_4009', 'Namibia_4010', 'Namibia_4011', 'Namibia_4012', 'Namibia_4013', 'Namibia_4014', 'Namibia_4015', 'Namibia_4016', 'Namibia_4017', 'Netherlands_4001', 'Netherlands_4002', 'Netherlands_4003', 'Netherlands_4004', 'Netherlands_4005', 'Netherlands_4006', 'Netherlands_4007', 'Netherlands_4008', 'Netherlands_4009', 'Netherlands_4010', 'Netherlands_4011', 'Netherlands_4012', 'Netherlands_4013', 'Netherlands_4014', 'Netherlands_4015', 'Netherlands_4016', 'Netherlands_4017', 'New Caledonia_4001', 'New Caledonia_4002', 'New Caledonia_4003', 'New Caledonia_4004', 'New Caledonia_4005', 'New Caledonia_4006', 'New Caledonia_4007', 'New Caledonia_4008', 'New Caledonia_4009', 'New Caledonia_4010', 'New Caledonia_4011', 'New Caledonia_4012', 'New Caledonia_4013', 'New Caledonia_4014', 'New Caledonia_4015', 'New Caledonia_4016', 'New Caledonia_4017', 'New Zealand_4001', 'New Zealand_4002', 'New Zealand_4003', 'New Zealand_4004', 'New Zealand_4005', 'New Zealand_4006', 'New Zealand_4007', 'New Zealand_4008', 'New Zealand_4009', 'New Zealand_4010', 'New Zealand_4011', 'New Zealand_4012', 'New Zealand_4013', 'New Zealand_4014', 'New Zealand_4015', 'New Zealand_4016', 'New Zealand_4017', 'Nicaragua_4001', 'Nicaragua_4002', 'Nicaragua_4003', 'Nicaragua_4004', 'Nicaragua_4005', 'Nicaragua_4006', 'Nicaragua_4007', 'Nicaragua_4008', 'Nicaragua_4009', 'Nicaragua_4010', 'Nicaragua_4011', 'Nicaragua_4012', 'Nicaragua_4013', 'Nicaragua_4014', 'Nicaragua_4015', 'Nicaragua_4016', 'Nicaragua_4017', 'Niger_4001', 'Niger_4002', 'Niger_4003', 'Niger_4004', 'Niger_4005', 'Niger_4006', 'Niger_4007', 'Niger_4008', 'Niger_4009', 'Niger_4010', 'Niger_4011', 'Niger_4012', 'Niger_4013', 'Niger_4014', 'Niger_4015', 'Niger_4016', 'Niger_4017', 'Nigeria_4001', 'Nigeria_4002', 'Nigeria_4003', 'Nigeria_4004', 'Nigeria_4005', 'Nigeria_4006', 'Nigeria_4007', 'Nigeria_4008', 'Nigeria_4009', 'Nigeria_4010', 'Nigeria_4011', 'Nigeria_4012', 'Nigeria_4013', 'Nigeria_4014', 'Nigeria_4015', 'Nigeria_4016', 'Nigeria_4017', 'Norway_4001', 'Norway_4002', 'Norway_4003', 'Norway_4004', 'Norway_4005', 'Norway_4006', 'Norway_4007', 'Norway_4008', 'Norway_4009', 'Norway_4010', 'Norway_4011', 'Norway_4012', 'Norway_4013', 'Norway_4014', 'Norway_4015', 'Norway_4016', 'Norway_4017', 'Oman_4001', 'Oman_4002', 'Oman_4003', 'Oman_4004', 'Oman_4005', 'Oman_4006', 'Oman_4007', 'Oman_4008', 'Oman_4009', 'Oman_4010', 'Oman_4011', 'Oman_4012', 'Oman_4013', 'Oman_4014', 'Oman_4015', 'Oman_4016', 'Oman_4017', 'Pakistan_4001', 'Pakistan_4002', 'Pakistan_4003', 'Pakistan_4004', 'Pakistan_4005', 'Pakistan_4006', 'Pakistan_4007', 'Pakistan_4008', 'Pakistan_4009', 'Pakistan_4010', 'Pakistan_4011', 'Pakistan_4012', 'Pakistan_4013', 'Pakistan_4014', 'Pakistan_4015', 'Pakistan_4016', 'Pakistan_4017', 'Palestine, State of_4001', 'Palestine, State of_4002', 'Palestine, State of_4003', 'Palestine, State of_4004', 'Palestine, State of_4005', 'Palestine, State of_4006', 'Palestine, State of_4007', 'Palestine, State of_4008', 'Palestine, State of_4009', 'Palestine, State of_4010', 'Palestine, State of_4011', 'Palestine, State of_4012', 'Palestine, State of_4013', 'Palestine, State of_4014', 'Palestine, State of_4015', 'Palestine, State of_4016', 'Palestine, State of_4017', 'Paraguay_4001', 'Paraguay_4002', 'Paraguay_4003', 'Paraguay_4004', 'Paraguay_4005', 'Paraguay_4006', 'Paraguay_4007', 'Paraguay_4008', 'Paraguay_4009', 'Paraguay_4010', 'Paraguay_4011', 'Paraguay_4012', 'Paraguay_4013', 'Paraguay_4014', 'Paraguay_4015', 'Paraguay_4016', 'Paraguay_4017', 'Peru_4001', 'Peru_4002', 'Peru_4003', 'Peru_4004', 'Peru_4005', 'Peru_4006', 'Peru_4007', 'Peru_4008', 'Peru_4009', 'Peru_4010', 'Peru_4011', 'Peru_4012', 'Peru_4013', 'Peru_4014', 'Peru_4015', 'Peru_4016', 'Peru_4017', 'Philippines_4001', 'Philippines_4002', 'Philippines_4003', 'Philippines_4004', 'Philippines_4005', 'Philippines_4006', 'Philippines_4007', 'Philippines_4008', 'Philippines_4009', 'Philippines_4010', 'Philippines_4011', 'Philippines_4012', 'Philippines_4013', 'Philippines_4014', 'Philippines_4015', 'Philippines_4016', 'Philippines_4017', 'Poland_4001', 'Poland_4002', 'Poland_4003', 'Poland_4004', 'Poland_4005', 'Poland_4006', 'Poland_4007', 'Poland_4008', 'Poland_4009', 'Poland_4010', 'Poland_4011', 'Poland_4012', 'Poland_4013', 'Poland_4014', 'Poland_4015', 'Poland_4016', 'Poland_4017', 'Portugal_4001', 'Portugal_4002', 'Portugal_4003', 'Portugal_4004', 'Portugal_4005', 'Portugal_4006', 'Portugal_4007', 'Portugal_4008', 'Portugal_4009', 'Portugal_4010', 'Portugal_4011', 'Portugal_4012', 'Portugal_4013', 'Portugal_4014', 'Portugal_4015', 'Portugal_4016', 'Portugal_4017', 'Qatar_4001', 'Qatar_4002', 'Qatar_4003', 'Qatar_4004', 'Qatar_4005', 'Qatar_4006', 'Qatar_4007', 'Qatar_4008', 'Qatar_4009', 'Qatar_4010', 'Qatar_4011', 'Qatar_4012', 'Qatar_4013', 'Qatar_4014', 'Qatar_4015', 'Qatar_4016', 'Qatar_4017', 'Romania_4001', 'Romania_4002', 'Romania_4003', 'Romania_4004', 'Romania_4005', 'Romania_4006', 'Romania_4007', 'Romania_4008', 'Romania_4009', 'Romania_4010', 'Romania_4011', 'Romania_4012', 'Romania_4013', 'Romania_4014', 'Romania_4015', 'Romania_4016', 'Romania_4017', 'Russian Federation_4001', 'Russian Federation_4002', 'Russian Federation_4003', 'Russian Federation_4004', 'Russian Federation_4005', 'Russian Federation_4006', 'Russian Federation_4007', 'Russian Federation_4008', 'Russian Federation_4009', 'Russian Federation_4010', 'Russian Federation_4011', 'Russian Federation_4012', 'Russian Federation_4013', 'Russian Federation_4014', 'Russian Federation_4015', 'Russian Federation_4016', 'Russian Federation_4017', 'Rwanda_4001', 'Rwanda_4002', 'Rwanda_4003', 'Rwanda_4004', 'Rwanda_4005', 'Rwanda_4006', 'Rwanda_4007', 'Rwanda_4008', 'Rwanda_4009', 'Rwanda_4010', 'Rwanda_4011', 'Rwanda_4012', 'Rwanda_4013', 'Rwanda_4014', 'Rwanda_4015', 'Rwanda_4016', 'Rwanda_4017', 'Saint Vincent and the Grenadines_4001', 'Saint Vincent and the Grenadines_4002', 'Saint Vincent and the Grenadines_4003', 'Saint Vincent and the Grenadines_4004', 'Saint Vincent and the Grenadines_4005', 'Saint Vincent and the Grenadines_4006', 'Saint Vincent and the Grenadines_4007', 'Saint Vincent and the Grenadines_4008', 'Saint Vincent and the Grenadines_4009', 'Saint Vincent and the Grenadines_4010', 'Saint Vincent and the Grenadines_4011', 'Saint Vincent and the Grenadines_4012', 'Saint Vincent and the Grenadines_4013', 'Saint Vincent and the Grenadines_4014', 'Saint Vincent and the Grenadines_4015', 'Saint Vincent and the Grenadines_4016', 'Saint Vincent and the Grenadines_4017', 'Samoa_4001', 'Samoa_4002', 'Samoa_4003', 'Samoa_4004', 'Samoa_4005', 'Samoa_4006', 'Samoa_4007', 'Samoa_4008', 'Samoa_4009', 'Samoa_4010', 'Samoa_4011', 'Samoa_4012', 'Samoa_4013', 'Samoa_4014', 'Samoa_4015', 'Samoa_4016', 'Samoa_4017', 'Saudi Arabia_4001', 'Saudi Arabia_4002', 'Saudi Arabia_4003', 'Saudi Arabia_4004', 'Saudi Arabia_4005', 'Saudi Arabia_4006', 'Saudi Arabia_4007', 'Saudi Arabia_4008', 'Saudi Arabia_4009', 'Saudi Arabia_4010', 'Saudi Arabia_4011', 'Saudi Arabia_4012', 'Saudi Arabia_4013', 'Saudi Arabia_4014', 'Saudi Arabia_4015', 'Saudi Arabia_4016', 'Saudi Arabia_4017', 'Serbia_4001', 'Serbia_4002', 'Serbia_4003', 'Serbia_4004', 'Serbia_4005', 'Serbia_4006', 'Serbia_4007', 'Serbia_4008', 'Serbia_4009', 'Serbia_4010', 'Serbia_4011', 'Serbia_4012', 'Serbia_4013', 'Serbia_4014', 'Serbia_4015', 'Serbia_4016', 'Serbia_4017', 'Seychelles_4001', 'Seychelles_4002', 'Seychelles_4003', 'Seychelles_4004', 'Seychelles_4005', 'Seychelles_4006', 'Seychelles_4007', 'Seychelles_4008', 'Seychelles_4009', 'Seychelles_4010', 'Seychelles_4011', 'Seychelles_4012', 'Seychelles_4013', 'Seychelles_4014', 'Seychelles_4015', 'Seychelles_4016', 'Seychelles_4017', 'Singapore_4001', 'Singapore_4002', 'Singapore_4003', 'Singapore_4004', 'Singapore_4005', 'Singapore_4006', 'Singapore_4007', 'Singapore_4008', 'Singapore_4009', 'Singapore_4010', 'Singapore_4011', 'Singapore_4012', 'Singapore_4013', 'Singapore_4014', 'Singapore_4015', 'Singapore_4016', 'Singapore_4017', 'Slovakia_4001', 'Slovakia_4002', 'Slovakia_4003', 'Slovakia_4004', 'Slovakia_4005', 'Slovakia_4006', 'Slovakia_4007', 'Slovakia_4008', 'Slovakia_4009', 'Slovakia_4010', 'Slovakia_4011', 'Slovakia_4012', 'Slovakia_4013', 'Slovakia_4014', 'Slovakia_4015', 'Slovakia_4016', 'Slovakia_4017', 'Slovenia_4001', 'Slovenia_4002', 'Slovenia_4003', 'Slovenia_4004', 'Slovenia_4005', 'Slovenia_4006', 'Slovenia_4007', 'Slovenia_4008', 'Slovenia_4009', 'Slovenia_4010', 'Slovenia_4011', 'Slovenia_4012', 'Slovenia_4013', 'Slovenia_4014', 'Slovenia_4015', 'Slovenia_4016', 'Slovenia_4017', 'Solomon Islands_4001', 'Solomon Islands_4002', 'Solomon Islands_4003', 'Solomon Islands_4004', 'Solomon Islands_4005', 'Solomon Islands_4006', 'Solomon Islands_4007', 'Solomon Islands_4008', 'Solomon Islands_4009', 'Solomon Islands_4010', 'Solomon Islands_4011', 'Solomon Islands_4012', 'Solomon Islands_4013', 'Solomon Islands_4014', 'Solomon Islands_4015', 'Solomon Islands_4016', 'Solomon Islands_4017', 'South Africa_4001', 'South Africa_4002', 'South Africa_4003', 'South Africa_4004', 'South Africa_4005', 'South Africa_4006', 'South Africa_4007', 'South Africa_4008', 'South Africa_4009', 'South Africa_4010', 'South Africa_4011', 'South Africa_4012', 'South Africa_4013', 'South Africa_4014', 'South Africa_4015', 'South Africa_4016', 'South Africa_4017', 'Spain_4001', 'Spain_4002', 'Spain_4003', 'Spain_4004', 'Spain_4005', 'Spain_4006', 'Spain_4007', 'Spain_4008', 'Spain_4009', 'Spain_4010', 'Spain_4011', 'Spain_4012', 'Spain_4013', 'Spain_4014', 'Spain_4015', 'Spain_4016', 'Spain_4017', 'Sweden_4001', 'Sweden_4002', 'Sweden_4003', 'Sweden_4004', 'Sweden_4005', 'Sweden_4006', 'Sweden_4007', 'Sweden_4008', 'Sweden_4009', 'Sweden_4010', 'Sweden_4011', 'Sweden_4012', 'Sweden_4013', 'Sweden_4014', 'Sweden_4015', 'Sweden_4016', 'Sweden_4017', 'Switzerland_4001', 'Switzerland_4002', 'Switzerland_4003', 'Switzerland_4004', 'Switzerland_4005', 'Switzerland_4006', 'Switzerland_4007', 'Switzerland_4008', 'Switzerland_4009', 'Switzerland_4010', 'Switzerland_4011', 'Switzerland_4012', 'Switzerland_4013', 'Switzerland_4014', 'Switzerland_4015', 'Switzerland_4016', 'Switzerland_4017', 'Taipei, Chinese_4001', 'Taipei, Chinese_4002', 'Taipei, Chinese_4003', 'Taipei, Chinese_4004', 'Taipei, Chinese_4005', 'Taipei, Chinese_4006', 'Taipei, Chinese_4007', 'Taipei, Chinese_4008', 'Taipei, Chinese_4009', 'Taipei, Chinese_4010', 'Taipei, Chinese_4011', 'Taipei, Chinese_4012', 'Taipei, Chinese_4013', 'Taipei, Chinese_4014', 'Taipei, Chinese_4015', 'Taipei, Chinese_4016', 'Taipei, Chinese_4017', 'Tanzania, United Republic of_4001', 'Tanzania, United Republic of_4002', 'Tanzania, United Republic of_4003', 'Tanzania, United Republic of_4004', 'Tanzania, United Republic of_4005', 'Tanzania, United Republic of_4006', 'Tanzania, United Republic of_4007', 'Tanzania, United Republic of_4008', 'Tanzania, United Republic of_4009', 'Tanzania, United Republic of_4010', 'Tanzania, United Republic of_4011', 'Tanzania, United Republic of_4012', 'Tanzania, United Republic of_4013', 'Tanzania, United Republic of_4014', 'Tanzania, United Republic of_4015', 'Tanzania, United Republic of_4016', 'Tanzania, United Republic of_4017', 'Thailand_4001', 'Thailand_4002', 'Thailand_4003', 'Thailand_4004', 'Thailand_4005', 'Thailand_4006', 'Thailand_4007', 'Thailand_4008', 'Thailand_4009', 'Thailand_4010', 'Thailand_4011', 'Thailand_4012', 'Thailand_4013', 'Thailand_4014', 'Thailand_4015', 'Thailand_4016', 'Thailand_4017', 'Togo_4001', 'Togo_4002', 'Togo_4003', 'Togo_4004', 'Togo_4005', 'Togo_4006', 'Togo_4007', 'Togo_4008', 'Togo_4009', 'Togo_4010', 'Togo_4011', 'Togo_4012', 'Togo_4013', 'Togo_4014', 'Togo_4015', 'Togo_4016', 'Togo_4017', 'Trinidad and Tobago_4001', 'Trinidad and Tobago_4002', 'Trinidad and Tobago_4003', 'Trinidad and Tobago_4004', 'Trinidad and Tobago_4005', 'Trinidad and Tobago_4006', 'Trinidad and Tobago_4007', 'Trinidad and Tobago_4008', 'Trinidad and Tobago_4009', 'Trinidad and Tobago_4010', 'Trinidad and Tobago_4011', 'Trinidad and Tobago_4012', 'Trinidad and Tobago_4013', 'Trinidad and Tobago_4014', 'Trinidad and Tobago_4015', 'Trinidad and Tobago_4016', 'Trinidad and Tobago_4017', 'Tunisia_4001', 'Tunisia_4002', 'Tunisia_4003', 'Tunisia_4004', 'Tunisia_4005', 'Tunisia_4006', 'Tunisia_4007', 'Tunisia_4008', 'Tunisia_4009', 'Tunisia_4010', 'Tunisia_4011', 'Tunisia_4012', 'Tunisia_4013', 'Tunisia_4014', 'Tunisia_4015', 'Tunisia_4016', 'Tunisia_4017', 'Turkey_4001', 'Turkey_4002', 'Turkey_4003', 'Turkey_4004', 'Turkey_4005', 'Turkey_4006', 'Turkey_4007', 'Turkey_4008', 'Turkey_4009', 'Turkey_4010', 'Turkey_4011', 'Turkey_4012', 'Turkey_4013', 'Turkey_4014', 'Turkey_4015', 'Turkey_4016', 'Turkey_4017', 'Uganda_4001', 'Uganda_4002', 'Uganda_4003', 'Uganda_4004', 'Uganda_4005', 'Uganda_4006', 'Uganda_4007', 'Uganda_4008', 'Uganda_4009', 'Uganda_4010', 'Uganda_4011', 'Uganda_4012', 'Uganda_4013', 'Uganda_4014', 'Uganda_4015', 'Uganda_4016', 'Uganda_4017', 'United Kingdom_4001', 'United Kingdom_4002', 'United Kingdom_4003', 'United Kingdom_4004', 'United Kingdom_4005', 'United Kingdom_4006', 'United Kingdom_4007', 'United Kingdom_4008', 'United Kingdom_4009', 'United Kingdom_4010', 'United Kingdom_4011', 'United Kingdom_4012', 'United Kingdom_4013', 'United Kingdom_4014', 'United Kingdom_4015', 'United Kingdom_4016', 'United Kingdom_4017', 'United States of America_4001', 'United States of America_4002', 'United States of America_4003', 'United States of America_4004', 'United States of America_4005', 'United States of America_4006', 'United States of America_4007', 'United States of America_4008', 'United States of America_4009', 'United States of America_4010', 'United States of America_4011', 'United States of America_4012', 'United States of America_4013', 'United States of America_4014', 'United States of America_4015', 'United States of America_4016', 'United States of America_4017', 'Uruguay_4001', 'Uruguay_4002', 'Uruguay_4003', 'Uruguay_4004', 'Uruguay_4005', 'Uruguay_4006', 'Uruguay_4007', 'Uruguay_4008', 'Uruguay_4009', 'Uruguay_4010', 'Uruguay_4011', 'Uruguay_4012', 'Uruguay_4013', 'Uruguay_4014', 'Uruguay_4015', 'Uruguay_4016', 'Uruguay_4017', 'Venezuela, Bolivarian Republic of_4001', 'Venezuela, Bolivarian Republic of_4002', 'Venezuela, Bolivarian Republic of_4003', 'Venezuela, Bolivarian Republic of_4004', 'Venezuela, Bolivarian Republic of_4005', 'Venezuela, Bolivarian Republic of_4006', 'Venezuela, Bolivarian Republic of_4007', 'Venezuela, Bolivarian Republic of_4008', 'Venezuela, Bolivarian Republic of_4009', 'Venezuela, Bolivarian Republic of_4010', 'Venezuela, Bolivarian Republic of_4011', 'Venezuela, Bolivarian Republic of_4012', 'Venezuela, Bolivarian Republic of_4013', 'Venezuela, Bolivarian Republic of_4014', 'Venezuela, Bolivarian Republic of_4015', 'Venezuela, Bolivarian Republic of_4016', 'Venezuela, Bolivarian Republic of_4017', 'Zambia_4001', 'Zambia_4002', 'Zambia_4003', 'Zambia_4004', 'Zambia_4005', 'Zambia_4006', 'Zambia_4007', 'Zambia_4008', 'Zambia_4009', 'Zambia_4010', 'Zambia_4011', 'Zambia_4012', 'Zambia_4013', 'Zambia_4014', 'Zambia_4015', 'Zambia_4016', 'Zambia_4017', 'Zimbabwe_4001', 'Zimbabwe_4002', 'Zimbabwe_4003', 'Zimbabwe_4004', 'Zimbabwe_4005', 'Zimbabwe_4006', 'Zimbabwe_4007', 'Zimbabwe_4008', 'Zimbabwe_4009', 'Zimbabwe_4010', 'Zimbabwe_4011', 'Zimbabwe_4012', 'Zimbabwe_4013', 'Zimbabwe_4014', 'Zimbabwe_4015', 'Zimbabwe_4016', 'Zimbabwe_4017', 'synthesis_seq']\n",
            "Data(x=[12, 2330], y=[12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rv5DjHyrQor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, rnn_num_layers=1, input_feature_len=1, sequence_len=168, hidden_size=100, bidirectional=False):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.sequence_len = sequence_len\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_feature_len = input_feature_len\n",
        "        self.num_layers = rnn_num_layers\n",
        "        self.rnn_directions = 2 if bidirectional else 1\n",
        "        self.lstm = nn.LSTM(\n",
        "            num_layers = rnn_num_layers,\n",
        "            input_size=input_feature_len,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        \n",
        "    def forward(self, input_seq):\n",
        "      \"\"\"\n",
        "      if torch.cuda.is_available():\n",
        "        print('cuda available')\n",
        "        ht = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size).cuda()\n",
        "        ct = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size).cuda()\n",
        "      else:\n",
        "        ht = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size)\n",
        "        ct = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size)\n",
        "        \"\"\"\n",
        "      ht = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size).to(device)\n",
        "      ct = torch.zeros(self.num_layers * self.rnn_directions, input_seq.size(0) , self.hidden_size).to(device)\n",
        "\n",
        "      if input_seq.ndim < 3:\n",
        "          input_seq.unsqueeze_(2)\n",
        "      lstm_out, ht_ct= self.lstm(input_seq, (ht, ct))\n",
        "      if self.rnn_directions > 1:\n",
        "          lstm_out = lstm_out.view(input_seq.size(0), self.sequence_len, self.rnn_directions, self.hidden_size)\n",
        "          lstm_out = torch.sum(lstm_out, axis=2)\n",
        "      return lstm_out, ht_ct[0].squeeze(0), ht_ct[1].squeeze(0)\n",
        "   \n",
        "class AttentionDecoderCell(nn.Module):\n",
        "    def __init__(self, input_feature_len, hidden_size, sequence_len, decoder_output_size):\n",
        "        super(AttentionDecoderCell, self).__init__()\n",
        "        # attention - inputs - (decoder_inputs, prev_hidden)\n",
        "        self.attention_linear = nn.Linear(hidden_size + input_feature_len, sequence_len)\n",
        "        # attention_combine - inputs - (decoder_inputs, attention * encoder_outputs)\n",
        "        self.decoder_rnn_cell = nn.LSTMCell(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, decoder_output_size)\n",
        "        \n",
        "    def forward(self, encoder_output, prev_hidden, prev_ct, y):\n",
        "      attention_input = torch.cat((prev_hidden, y), axis=1)\n",
        "      attention_weights = F.softmax(self.attention_linear(attention_input)).unsqueeze(1)\n",
        "      attention_combine = torch.bmm(attention_weights, encoder_output).squeeze(1)\n",
        "      rnn_hidden, prev_ct = self.decoder_rnn_cell(attention_combine, (prev_hidden, prev_ct))\n",
        "      output = self.out(rnn_hidden)\n",
        "      return output, rnn_hidden, prev_ct\n",
        "    \n",
        "\n",
        "class EncoderDecoderWrapper():\n",
        "    def __init__(self, encoder, decoder_cell, optimizers = None,  output_size=3, teacher_forcing=0.3, learning_rate = 0.01):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder_cell = decoder_cell\n",
        "        self.output_size = output_size\n",
        "        self.teacher_forcing = teacher_forcing\n",
        "        \n",
        "        if optimizers == None:\n",
        "          self.optimizers_init(learning_rate)\n",
        "        \n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder_cell.train()\n",
        "        \n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder_cell.eval()\n",
        "        \n",
        "    def state_dict(self):\n",
        "        return {\n",
        "            'encoder': self.encoder.state_dict(),\n",
        "            'decoder_cell': self.decoder_cell.state_dict()\n",
        "        }\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.encoder.load_state_dict(state_dict['encoder'])\n",
        "        self.decoder_cell.load_state_dict(state_dict['decoder_cell'])\n",
        "\n",
        "    def __call__(self, input_seq, batch_size, yb=None):\n",
        "      input_seq = input_seq.to(device)\n",
        "      encoder_output, encoder_hidden, encoder_ct = self.encoder(input_seq)\n",
        "      prev_hidden = encoder_hidden\n",
        "      prev_ct = encoder_ct\n",
        "      \"\"\"\n",
        "      if torch.cuda.is_available():\n",
        "          outputs = torch.zeros(1, self.output_size, device='cuda')\n",
        "      else:\n",
        "          outputs = torch.zeros(1, self.output_size)\n",
        "          \"\"\"\n",
        "      outputs = torch.zeros(batch_size, self.output_size).to(device)\n",
        "      y_prev = torch.zeros(batch_size,1).to(device)\n",
        "      for i in range(self.output_size):\n",
        "          if (yb is not None) and (i > 0) and (torch.rand(1) < self.teacher_forcing):\n",
        "              y_prev = yb[:, i].unsqueeze(1)\n",
        "          rnn_output, prev_hidden, prev_ct = self.decoder_cell(encoder_output, prev_hidden, prev_ct, y_prev)\n",
        "          y_prev = rnn_output\n",
        "          outputs[:, i] = rnn_output.squeeze(1)\n",
        "      return outputs\n",
        "    \n",
        "    def step(self):\n",
        "      self.optimizers[0].step()\n",
        "      self.optimizers[1].step()\n",
        "\n",
        "    def optimizers_init(self, learning_rate):\n",
        "      enc_optim = torch.optim.Adam(self.encoder.parameters(), lr = learning_rate)\n",
        "      dec_optim = torch.optim.Adam(self.decoder_cell.parameters(), lr = learning_rate)\n",
        "      self.optimizers = (\n",
        "          enc_optim, dec_optim\n",
        "      )\n",
        "\n",
        "    def optimizers_zero_grad(self):\n",
        "      self.optimizers[0].zero_grad()\n",
        "      self.optimizers[1].zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdLwz0jmT_bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "configuration = {\n",
        "    'input_sequence_len' : 12,\n",
        "    'output_sequence_len' : 12,\n",
        "    'training_batch_size' : 1,\n",
        "    'training_dataset_length' :32768,\n",
        "    'validation_batch_size' : 1,\n",
        "    'yhat_size' : 1,\n",
        "    'feature_len' :len(features)-1 ,\n",
        "    'output_size' : 1,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZBmfYEtiWem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_rnn(input_tensor, target_tensor, model, criterion,batch_size):\n",
        "  input_tensor = input_tensor.type(torch.FloatTensor)\n",
        "  target_tensor= target_tensor.type(torch.FloatTensor)\n",
        "  target_tensor = target_tensor.to(device)\n",
        "  model.train()\n",
        "  model.optimizers_zero_grad()\n",
        "  loss = 0\n",
        "  yhat = model(input_tensor, batch_size) #remove in batch training mode\n",
        "  loss += criterion(yhat.squeeze(),target_tensor.squeeze() )\n",
        "  loss.backward()\n",
        "  model.step()\n",
        "  return loss.item() / configuration['input_sequence_len']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bb9yefOKZrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_rnn(input_tensor, target_tensor, model, criterion,batch_size):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = input_tensor.type(torch.FloatTensor)\n",
        "    target_tensor= target_tensor.type(torch.FloatTensor)\n",
        "    target_tensor = target_tensor.to(device)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    yhat = model(input_tensor, batch_size) #remove in batch training mode\n",
        "    val_loss += criterion(yhat.squeeze(),target_tensor.squeeze() )\n",
        "  return val_loss.item() / configuration['input_sequence_len']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp8Lke6HhkBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def train_batch(dataloader, model, criterion):\n",
        "  loss = 0\n",
        "  for batch in dataloader:\n",
        "    batch.x = batch.x.cuda(device=0)\n",
        "    batch.y = batch.y.cuda(device=0)\n",
        "    batch_size = int(len(batch.x)/configuration['input_sequence_len'])\n",
        "    loss += (train_rnn(batch.x.view(batch_size, configuration['input_sequence_len'] , configuration['feature_len']),\n",
        "                       batch.y.view(batch_size, configuration['output_sequence_len']), model,\n",
        "                       criterion, batch_size))/ batch_size\n",
        "  return loss / len(dataloader)\n",
        "\n",
        "def batch_evaluate(dataloader, model, criterion, output_sequence_len):\n",
        "  loss = 0\n",
        "  for batch in dataloader:\n",
        "    batch.x = batch.x.to(device)\n",
        "    batch.y = batch.y.to(device)\n",
        "    batch_size = int(len(batch.x)/configuration['input_sequence_len'])\n",
        "    loss += (evaluate_rnn(batch.x.view(batch_size, configuration['input_sequence_len'] , configuration['feature_len']),\n",
        "                          batch.y.view(batch_size, configuration['output_sequence_len']), model,\n",
        "                     criterion, batch_size))/ batch_size\n",
        "  return loss / len(dataloader)\n",
        "\n",
        "def trainValidationIters(train_dataloader,validation_dataloader, model, \n",
        "                        criterion ,n_iters, output_sequence_len,input_sequence_len,\n",
        "                        current_model_dir, model_id,session_id,print_every=10, plot_every=100,save_every = 100,check_validation_every=1000,id=\"train_id_\"):\n",
        "\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  \n",
        "  # Reset every print_every # Reset every plot_every\n",
        "  print_training_loss_total = 0\n",
        "  plot_training_loss_total = 0\n",
        "  print_validation_loss_total = 0\n",
        "  plot_validation_loss_total = 0\n",
        "  save_validation_loss_total = 0\n",
        "  save_training_loss_total = 0\n",
        "      \n",
        "  losses = []\n",
        "  models_temp = dict()\n",
        "  optim_temp = dict()\n",
        "  for iter in range(1, n_iters + 1):\n",
        "    training_loss = train_batch(train_dataloader,model, criterion )\n",
        "    validation_loss = batch_evaluate(validation_dataloader, model,\n",
        "                                      criterion,configuration['output_sequence_len'] )\n",
        "\n",
        "    models_temp[validation_loss] = copy.deepcopy(model) #Save the model\n",
        "\n",
        "    print_training_loss_total += training_loss\n",
        "    plot_training_loss_total += training_loss\n",
        "    print_validation_loss_total += validation_loss\n",
        "    plot_validation_loss_total += validation_loss\n",
        "    \n",
        "    save_training_loss_total += training_loss\n",
        "    save_validation_loss_total += validation_loss\n",
        "    \n",
        "    if iter % print_every == 0:\n",
        "        print_training_loss_avg = print_training_loss_total / print_every\n",
        "        print_training_loss_total = 0\n",
        "        print_validation_loss_avg = print_validation_loss_total / print_every\n",
        "        print_validation_loss_total = 0\n",
        "        print('%s (%d %d%%) training:%.15f validation:%.15f' % (timeSince(start, iter / n_iters),\n",
        "                                      iter, iter / n_iters * 100, print_training_loss_avg,print_validation_loss_avg, ))\n",
        "        print('train/validation difference : ',print_validation_loss_avg-print_training_loss_avg)\n",
        "      \n",
        "    if iter % save_every == 0:\n",
        "      save_training_loss_avg = save_training_loss_total / save_every\n",
        "      save_validation_loss_avg = plot_validation_loss_total / save_every\n",
        "      #'arch': args.arch,\n",
        "      \n",
        "    losses.append([training_loss, validation_loss])\n",
        "\n",
        "    if iter % check_validation_every == 0 and iter >= check_validation_every:\n",
        "      x = [0 , 1]\n",
        "      y = np.array(losses).T[1][-1*check_validation_every:].tolist()\n",
        "      y = [y[-1*check_validation_every:][0], y[-1*check_validation_every:][-1]]\n",
        "      m,b = np.polyfit(x, y, 1)\n",
        "      if m > 0 :\n",
        "        print(y)\n",
        "        print('Terminating the training at :',iter , )\n",
        "        break\n",
        "    torch.cuda.empty_cache()\n",
        "  return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZZdq4HYh2Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run all the candidate models\n",
        "def train_models( model,training_dataloader,learning_rate = 0.001,epochs = 2500, id= \"0\"):\n",
        "  #for i in model_configurations:\n",
        "  #  #instantiate the models\n",
        "  i = 1\n",
        "  if i != 0 :\n",
        "    print('skipping', i)\n",
        "    #continue\n",
        "  current_model_dir =  MODEL_DIR +'/'+str(i)\n",
        "  if os.path.exists(current_model_dir) == False:\n",
        "    os.mkdir(current_model_dir)\n",
        "  losses = trainValidationIters(training_dataloader,validation_dataloader,\n",
        "                    model,\n",
        "                    criterion, n_iters = epochs,output_sequence_len=configuration['output_sequence_len'],\n",
        "                    input_sequence_len=configuration['input_sequence_len'],\n",
        "                    current_model_dir=current_model_dir,  model_id = i,session_id = session_id,\n",
        "                    print_every=10, save_every = 100,check_validation_every=3000, id=id)\n",
        "  \n",
        "  print('Model ',str(i), ': trained and evaluated')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3sGikLKufvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_ROOT = PATH+'/input_mix/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHLM8BnLR9sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a new directory to store model results\n",
        "MODEL_PREFIX = '_hs'\n",
        "MODEL_DIR =  PATH + '/models'+MODEL_PREFIX\n",
        "dirs = []\n",
        "for f in listdir(MODEL_DIR):\n",
        "  if \"run\" in f:\n",
        "    dirs.append(int(f[3]))\n",
        "if len(dirs) > 0:\n",
        "  if os.path.exists(MODEL_DIR+\"/run\"+str(max(dirs)+1)) == False:\n",
        "    os.mkdir(MODEL_DIR+\"/run\"+str(max(dirs)+1))\n",
        "    MODEL_DIR = MODEL_DIR+\"/run\"+str(max(dirs)+1)\n",
        "else:\n",
        "  os.mkdir(MODEL_DIR+\"/run0\")\n",
        "  MODEL_DIR = MODEL_DIR+\"/run0\"\n",
        "\n",
        "if os.path.exists(MODEL_DIR)==False:\n",
        "  os.mkdir(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI2ISDr1GMqL",
        "colab_type": "code",
        "outputId": "d0f850c5-2241-4ba1-f7cd-ac6c19e8da27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "session_id = 1\n",
        "INPUT_ROOT = PATH+'/input_mix/test'\n",
        "CLEANED_DIR = '/cleaned'\n",
        "PROCESSED_DIR = '/processed_window_hs_seq2seq_'+ str(configuration['input_sequence_len'])+ '_'+str(configuration['output_sequence_len'])\n",
        "\n",
        "validation_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['output_sequence_len'])\n",
        "validation_dataset = validation_dataset.shuffle()\n",
        "validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n",
        "\n",
        "encoder = RNNEncoder(rnn_num_layers=1, input_feature_len=configuration['feature_len'], sequence_len=configuration['input_sequence_len'], hidden_size=100, bidirectional=False)\n",
        "decoder_cell = AttentionDecoderCell(input_feature_len = 1, hidden_size = 100, sequence_len = configuration['output_sequence_len'], decoder_output_size= 1)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder_cell.to(device)\n",
        "\n",
        "model = EncoderDecoderWrapper(encoder, decoder_cell, output_size=12, teacher_forcing=0.3)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "dirs = ['/'+f for f in listdir(INPUT_ROOT+'/training') if \"cleaned_window_hs\" in f]\n",
        "print(dirs)\n",
        "\n",
        "for loop in range(50):\n",
        "  print('loop :', loop)\n",
        "  i = 0\n",
        "  for CLEANED_DIR  in dirs:\n",
        "    PROCESSED_DIR = CLEANED_DIR.split( sep=\"_\")[0]+'_seq2seq_' +str(configuration['input_sequence_len'])+'_'+str(configuration['output_sequence_len'])\n",
        "    print(INPUT_ROOT+PROCESSED_DIR)\n",
        "    \n",
        "    training_dataset = Seq2SeqDataSet(INPUT_ROOT+'/training', configuration['input_sequence_len'], configuration['input_sequence_len'])\n",
        "    training_dataset = training_dataset.shuffle()\n",
        "    training_dataset = training_dataset[:1000]\n",
        "    training_dataset = training_dataset[:configuration['training_dataset_length']]\n",
        "    \n",
        "    training_dataloader = DataLoader(training_dataset,batch_size=configuration['training_batch_size'])\n",
        "    model =  train_models( model,learning_rate = 0.001, epochs = 3000,training_dataloader = training_dataloader,id= str(i) )\n",
        "    torch.save(model.encoder, PATH+'/models_rnn/encoder_latest.model')\n",
        "    torch.save(model.decoder_cell, PATH+'/models_rnn/encoder_latest.optim')\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/cleaned_window_hs']\n",
            "loop : 0\n",
            "/content/drive/My Drive/rubber/input_mix/test/cleaned_seq2seq_12_12\n",
            "skipping 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3m 47s (- 1133m 9s) (10 0%) training:0.008325649560227 validation:0.008109897116679\n",
            "train/validation difference :  -0.00021575244354833135\n",
            "7m 35s (- 1131m 36s) (20 0%) training:0.007028482950225 validation:0.006688924925295\n",
            "train/validation difference :  -0.0003395580249296176\n",
            "11m 27s (- 1133m 34s) (30 1%) training:0.007085892610390 validation:0.006925687300709\n",
            "train/validation difference :  -0.0001602053096804697\n",
            "15m 14s (- 1127m 24s) (40 1%) training:0.006626441231921 validation:0.006482445103094\n",
            "train/validation difference :  -0.00014399612882745605\n",
            "19m 1s (- 1122m 15s) (50 1%) training:0.006818876319410 validation:0.006807914759824\n",
            "train/validation difference :  -1.0961559585460738e-05\n",
            "22m 48s (- 1117m 52s) (60 2%) training:0.006612713559568 validation:0.006489322453791\n",
            "train/validation difference :  -0.00012339110577616694\n",
            "26m 36s (- 1113m 54s) (70 2%) training:0.006812821334203 validation:0.006803208933282\n",
            "train/validation difference :  -9.612400921055024e-06\n",
            "30m 22s (- 1108m 47s) (80 2%) training:0.006847571911345 validation:0.006992042953019\n",
            "train/validation difference :  0.00014447104167428226\n",
            "34m 7s (- 1103m 21s) (90 3%) training:0.006570674409735 validation:0.006546619530470\n",
            "train/validation difference :  -2.405487926520917e-05\n",
            "37m 53s (- 1098m 42s) (100 3%) training:0.006469435526627 validation:0.006603241308923\n",
            "train/validation difference :  0.00013380578229606633\n",
            "41m 39s (- 1094m 25s) (110 3%) training:0.006994012852228 validation:0.007163154238740\n",
            "train/validation difference :  0.00016914138651108468\n",
            "45m 24s (- 1089m 56s) (120 4%) training:0.007119895404102 validation:0.007028259570918\n",
            "train/validation difference :  -9.163583318362502e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo4X0Tk0kEKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}