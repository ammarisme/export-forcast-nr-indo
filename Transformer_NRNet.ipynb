{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Transformer_NRNet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"C83XgaWprQnn","colab_type":"code","outputId":"6593be5c-2cb5-4832-e6b4-f0ae4f8613fa","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589273512769,"user_tz":-330,"elapsed":21729,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["%matplotlib inline\n","!pip install torch-geometric \\\n","  torch-sparse==latest+cu101 \\\n","  torch-scatter==latest+cu101 \\\n","  torch-cluster==latest+cu101 \\\n","  -f https://pytorch-geometric.com/whl/torch-1.5.0.html"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n","Collecting torch-geometric\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/35/8a65fc0b685d916f5f70199d6ad6f19bb002dc3a547a3fe5b68d60047f3b/torch_geometric-1.4.3.tar.gz (129kB)\n","\r\u001b[K     |██▌                             | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 71kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.5MB/s \n","\u001b[?25hCollecting torch-sparse==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n","\u001b[K     |████████████████████████████████| 22.0MB 1.2MB/s \n","\u001b[?25hCollecting torch-scatter==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n","\u001b[K     |████████████████████████████████| 12.3MB 43.7MB/s \n","\u001b[?25hCollecting torch-cluster==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (18.2MB)\n","\u001b[K     |████████████████████████████████| 18.2MB 169kB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n","Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n","Collecting plyfile\n","  Downloading https://files.pythonhosted.org/packages/93/c8/cf47848cd4d661850e4a8e7f0fc4f7298515e06d0da7255ed08e5312d4aa/plyfile-0.7.2-py3-none-any.whl\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.3)\n","Collecting rdflib\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n","\u001b[K     |████████████████████████████████| 235kB 17.4MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (7.0.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.2.1)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.1.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n","Collecting isodate\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-1.4.3-cp36-none-any.whl size=234873 sha256=e64054dbb565f9fdd9a69cf94315abda88e34325e8559aef990338e59d3b489c\n","  Stored in directory: /root/.cache/pip/wheels/e2/c1/09/8693feee3f97e440d68b09abfca8b4c1e97150ace350b5003f\n","Successfully built torch-geometric\n","Installing collected packages: plyfile, isodate, rdflib, torch-geometric, torch-sparse, torch-scatter, torch-cluster\n","Successfully installed isodate-0.6.0 plyfile-0.7.2 rdflib-5.0.0 torch-cluster-1.5.4 torch-geometric-1.4.3 torch-scatter-2.0.4 torch-sparse-0.6.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gRUdaEpW2GsF","colab_type":"text"},"source":["!pip install torch-geometric \\\n","  torch-sparse==latest+cu101 \\\n","  torch-scatter==latest+cu101 \\\n","  torch-cluster==latest+cu101 \\\n","  -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n"]},{"cell_type":"code","metadata":{"id":"anKz91FjrQn3","colab_type":"code","outputId":"4965deef-af95-41b0-abe6-16cfdd7948d1","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589273519861,"user_tz":-330,"elapsed":28733,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import os\n","\n","import time\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch_geometric.data import Data, DataLoader, InMemoryDataset\n","\n","import numpy as np\n","from numpy import isnan\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","\n","from datetime import datetime\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('running on '+ str(device))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["running on cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bFsXKb1CzwSY","colab_type":"code","outputId":"49709ca1-89c2-42f5-9c6e-0718cf15307e","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1589273542814,"user_tz":-330,"elapsed":51630,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["from google.colab import drive\n","import os.path\n","from os import path\n","import sys\n","from os import listdir\n","from os.path import isfile, join\n","\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","PATH = '/content/drive/My Drive/rubber'\n","INPUT_ROOT = PATH+'/input_mix'\n","sys.path.append(PATH)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive not mounted, so nothing to flush and unmount.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HidtRwtG9CFu","colab_type":"code","colab":{}},"source":["class Seq2SeqDataSet(InMemoryDataset):\n","    def __init__(self, root, input_sequence, output_sequence, transform=None, pre_transform=None):\n","        super(Seq2SeqDataSet, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_dir(self):\n","      if os.path.exists(self.root+PROCESSED_DIR):\n","        return self.root+CLEANED_DIR #'/cleaned'\n","      else:\n","        os.mkdir(self.root+PROCESSED_DIR)\n","        return self.root+CLEANED_DIR#'/cleaned'\n","        \n","    @property\n","    def processed_dir(self):\n","      if os.path.exists(self.root+PROCESSED_DIR):\n","        return self.root+PROCESSED_DIR\n","      else:\n","        os.mkdir(self.root+PROCESSED_DIR)\n","        return self.root+PROCESSED_DIR\n","\n","    @property\n","    def raw_file_names(self):\n","      mypath = self.raw_dir\n","      filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","      return filenames\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['processed.dt']\n","\n","    def download(self):\n","        pass\n","    \n","    def process(self):\n","      output_size , output_sequence_len, input_sequence_len= configuration['output_size'],\\\n","      configuration['output_sequence_len'],\\\n","      configuration['input_sequence_len'],\n","        \n","      data_list = []\n","\n","      for raw_path in self.raw_paths:\n","        print(raw_path)\n","        df = pd.read_csv(raw_path)\n","        for synthetic_seq in df['synthesis_seq'].unique():\n","          synthetic_data = df[df['synthesis_seq']==synthetic_seq]\n","\n","          for country in synthetic_data['countryterritoryCode'].unique():\n","            country_data = synthetic_data[synthetic_data['countryterritoryCode'] == country]\n","                        \n","            del country_data['synthesis_seq']\n","            del country_data['countryterritoryCode']\n","\n","            country_data_i = country_data[:-output_sequence_len]\n","            country_data_array = country_data_i.to_numpy()\n","            feature_length = len(country_data.columns)\n","            x = country_data_array#[:feature_length].T\n","\n","            country_data_o = country_data[input_sequence_len:]\n","            #country_data_array = country_data_array.reshape(feature_length,len(country_data_i))\n","            country_data_array_y = np.array([country_data_o['NATURAL_RUBBER_EXPORT'].to_numpy()])\n","            country_data_array_y = country_data_array_y.reshape(output_size,len(country_data_o))\n","            y = country_data_array_y[:output_size].T\n","\n","            #print(country_data_array.shape)\n","            #print(country_data_array[0])\n","            \n","            \n","            #print(x.shape)\n","            sets =0\n","            x_list = []\n","            dict_x = dict()\n","            for i in range(input_sequence_len):\n","              array_len = (len(x)-i) - ((len(x)  - i)%input_sequence_len)+i\n","              if array_len <= 0:\n","                print('skipping')\n","                continue\n","              sets = int( array_len/ input_sequence_len)\n","              if sets <= 0:\n","                print('skipping')\n","                continue\n","              #print(len(x))\n","              #print('input seq : ', i , ' ', array_len , ' ',array_len-i , ' number of sets : ', sets)\n","              x_temp = x[i:array_len].T.reshape(sets, feature_length, input_sequence_len)\n","              uniq_keys = np.array([i+k*input_sequence_len for k in range(sets)])\n","              x_temp = x_temp.reshape(feature_length,sets,input_sequence_len)\n","              arrays_split = np.hsplit(x_temp,sets)\n","              dict_x.update(dict(zip(uniq_keys, arrays_split)))\n","\n","            dict_y = dict()\n","            y_list = []\n","            for i in range(output_sequence_len):\n","              array_len_y = (len(y)-i) - ((len(y)  - i)%output_sequence_len)+i\n","              if array_len_y <= 0:\n","                continue\n","              sets = int(array_len_y / output_sequence_len)\n","              if sets <= 0:\n","                continue\n","              y_temp = y[i:array_len_y].T.reshape(sets, output_size, output_sequence_len)\n","              #uniq_keys = np.array([i+(output_sequence_len*k) for k in range(output_sequence_len)])\n","              uniq_keys = np.array([i+k*output_sequence_len for k in range(sets)])\n","              y_temp = y_temp.reshape(output_size,sets,output_sequence_len)\n","              arrays_split = np.hsplit(y_temp,sets)\n","              dict_y.update(dict(zip(uniq_keys, arrays_split)))\n","\n","            temp_x_list = []\n","            mean = np.mean(country_data.to_numpy(), axis=0).T\n","            std = np.std(country_data.to_numpy(), axis=0).T\n","            #print(sorted(dict_x.keys()))\n","            for i in sorted(dict_x.keys()):\n","              x = dict_x[i].squeeze()\n","              #print(x.T[0][12])\n","              x = (x.T - mean) / std\n","              where_are_NaNs = isnan(x)\n","              x[where_are_NaNs] = 0\n","              temp_x_list.append(x)\n","\n","            temp_y_list  = [dict_y[i].T for i in sorted(dict_y.keys())]\n","\n","            #_country_code,popData2018\n","            xy_list = [Data(x = torch.from_numpy(features).type(torch.FloatTensor).squeeze()) for features in temp_x_list]\n","            \n","            mean_y = np.mean(y)\n","            std_y = np.std(y)\n","\n","            if configuration['output_sequence_len'] > 1 :\n","              for i in sorted(dict_y.keys()):\n","                temp_y_list[i] = temp_y_list[i].squeeze()\n","                norm_y = (temp_y_list[i] -mean_y)/std_y\n","                xy_list[i].y = torch.from_numpy(norm_y).squeeze()\n","            else:\n","              for i in sorted(dict_y.keys()):\n","                #print(type(temp_y_list[i].squeeze()))\n","                #print(temp_y_list[i].squeeze())\n","                norm_y = (temp_y_list[i] -mean_y)/std_y\n","                xy_list[i].y = torch.from_numpy(norm_y).squeeze(1).squeeze(0)\n","\n","            data_list += xy_list\n","        print('processed : '+ raw_path)\n","      data, slices = self.collate(data_list)\n","      torch.save((data, slices), self.processed_paths[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxT51PfpE5Ih","colab_type":"code","outputId":"5c698c5e-45f5-46ed-d9ec-7d7be3234daf","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1589273583657,"user_tz":-330,"elapsed":6710,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["configuration = {\n","    'input_sequence_len' : 6,\n","    'output_sequence_len' : 1,\n","    'training_batch_size' : 2048,\n","    'training_dataset_length' :32768,\n","    'validation_batch_size' : 1024,\n","    'yhat_size' : 1,\n","    'feature_len' : 21,\n","    'output_size' : 1,\n","}\n","\n","\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","\n","INPUT_ROOT = PATH+'/input_mix'\n","CLEANED_DIR = '/cleaned'\n","PROCESSED_DIR = '/processed_seq2seq_'+str(configuration['input_sequence_len'])+'_'+str(configuration['output_sequence_len'])\n","#os.rmdir(INPUT_ROOT+'/validation/'+PROCESSED_DIR)\n","\n","validation_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['output_sequence_len'])\n","validation_dataset = validation_dataset.shuffle()\n","validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n","print(validation_dataset[0])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Data(x=[6, 21], y=[1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Rv5DjHyrQor","colab_type":"code","colab":{}},"source":["class RNNModel(nn.Module):\n","    def __init__(self, input_size, parameter_sizes, repeats ,output_size):\n","        super(RNNModel, self).__init__()\n","        self.input_size = input_size\n","        self.repeater_input_size = parameter_sizes[0]\n","        self.hidden_size = parameter_sizes[1]\n","        self.repeats = repeats\n","        self.output_size = output_size\n","\n","        self.fc1 = nn.Linear(input_size, self.repeater_input_size)\n","        self.relu_activation = nn.ReLU()\n","\n","        self.layers = dict()\n","\n","        \n","        k = 0\n","        for i in range(repeats):\n","          i = i+k\n","          self.layers['fc_'+str(i)] = nn.Linear(self.repeater_input_size, self.hidden_size)\n","          self.layers['gru_'+str(i+1)] = nn.GRU(self.hidden_size, self.hidden_size)\n","          self.layers['fc_'+str(i+2)] = nn.Linear(self.hidden_size, self.repeater_input_size)\n","          k+=2\n","\n","        self.module_list = nn.ModuleDict(self.layers)\n","\n","        self.fc2 = nn.Linear(self.repeater_input_size, output_size)\n","        \n","    def forward(self, input, hidden):\n","      output = self.fc1(input)\n","      output = self.relu_activation(output)\n","\n","      k = 0\n","      for i in range(self.repeats):\n","        i = i+k\n","        output = self.layers['fc_'+str(i)](output)\n","        output = self.relu_activation(output)\n","\n","        self.layers['gru_'+str(i+1)].flatten_parameters()\n","        output, hidden[i-k] = self.layers['gru_'+str(i+1)](output, hidden[i-k])#should be different. check the nlp page\n","        output = self.relu_activation(output)\n","        hidden[i-k] = self.relu_activation(hidden[i-k])\n","\n","        output = self.layers['fc_'+str(i+2)](output)\n","        output = self.relu_activation(output)\n","        k +=2\n","\n","      output = self.fc2(output)\n","      #output = self.relu_activation(output)\n","\n","      return output, hidden\n","\n","    def initHidden(self, batch_size):\n","      hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n","      return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UE4BjcoWB5L5","colab_type":"code","colab":{}},"source":["class DecoderNet(nn.Module):\n","    def __init__(self, input_size, parameter_sizes, repeats ,output_size):\n","        super(DecoderNet, self).__init__()\n","        self.input_size = input_size\n","        self.repeater_input_size = parameter_sizes[0]\n","        self.hidden_size = parameter_sizes[1]\n","        self.repeats = repeats\n","        self.output_size = output_size\n","\n","        self.fc1 = nn.Linear(input_size, self.repeater_input_size)\n","        self.relu_activation = nn.ReLU()\n","\n","        self.layers = dict()\n","        \n","        k = 0\n","        for i in range(repeats):\n","          i = i+k\n","          self.layers['fc_'+str(i)] = nn.Linear(self.repeater_input_size, self.hidden_size)\n","          self.layers['gru_'+str(i+1)] = nn.GRU(self.hidden_size, self.hidden_size)\n","          self.layers['fc_'+str(i+2)] = nn.Linear(self.hidden_size, self.repeater_input_size)\n","          k+=2\n","\n","        self.module_list = nn.ModuleDict(self.layers)\n","\n","        self.fc2 = nn.Linear(self.repeater_input_size, output_size)\n","        \n","    def forward(self, input,hidden):\n","      output = self.fc1(input)\n","      output = self.relu_activation(output)\n","\n","      k = 0\n","      for i in range(self.repeats):\n","        i = i+k\n","        output = self.layers['fc_'+str(i)](output)\n","        output = self.relu_activation(output)\n","        self.layers['gru_'+str(i+1)].flatten_parameters()\n","        output, hidden[i-k] = self.layers['gru_'+str(i+1)](output, hidden[i-k])#should be different. check the nlp page\n","        output = self.relu_activation(output)\n","        hidden[i-k] = self.relu_activation(hidden[i-k])\n","\n","        output = self.layers['fc_'+str(i+2)](output)\n","        output = self.relu_activation(output)\n","        k +=2\n","\n","      output = self.fc2(output)\n","      \n","      return output, hidden\n","\n","#    def initHidden(self, batch_size):\n","#        return torch.zeros(1, batch_size, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZBmfYEtiWem","colab_type":"code","colab":{}},"source":["def train_rnn(input_tensor, target_tensor, rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer,  criterion,batch_size):\n","  #pass through the encoder\n","  input_tensor = input_tensor.type(torch.FloatTensor)\n","  target_tensor= target_tensor.type(torch.FloatTensor)\n","  input_tensor_length = len(input_tensor)\n","  \n","  rnn_model_hidden = [] #this status will be passed to the decoder\n","  for i in range(rnn_model.repeats):\n","    rnn_model_hidden.append(rnn_model.initHidden(batch_size))\n","\n","  encoder_outputs = torch.zeros([configuration['input_sequence_len'],input_tensor_length], dtype=torch.float32).to(device)\n","\n","  for ei in range(configuration['input_sequence_len']):\n","    #target_tensor_seq = target_tensor.view(configuration['output_sequence_len'], batch_size, configuration['yhat_size'])\n","    input_tensor_seq = input_tensor.view(configuration['input_sequence_len'],batch_size, configuration['feature_len'])[ei]\n","    input_tensor_seq = input_tensor_seq.view(1, batch_size, configuration['feature_len']).to(device)\n","    rnn_model_output, rnn_model_hidden = rnn_model(\n","        input_tensor_seq, rnn_model_hidden)\n","    encoder_outputs[ei]= rnn_model_output.squeeze()\n","\n","  #pass through the decoder  \n","  loss = 0\n","  decoder_model_optimizer.zero_grad()\n","  rnn_model_optimizer.zero_grad()\n","  encoder_outputs = torch.transpose(encoder_outputs, 0, 1).unsqueeze(0)\n","  \n","  decoder_output, decoder_hidden = decoder_model(encoder_outputs, rnn_model_hidden)\n","  target_tensor_seq= target_tensor.view(configuration['output_sequence_len'], batch_size, configuration['yhat_size'])\n","  loss += criterion(decoder_output.squeeze(), target_tensor_seq.squeeze().to(device))\n","  loss.backward()\n","\n","  rnn_model_optimizer.step()\n","  decoder_model_optimizer.step()\n","  \n","\n","  return loss.item() / configuration['input_sequence_len']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Bb9yefOKZrz","colab_type":"code","colab":{}},"source":["def evaluate_rnn(input_tensor, target_tensor, rnn_model, rnn_model_optimizer, decoder_model, decoder_optimizer, criterion,batch_size):\n","  with torch.no_grad():\n","    #pass through the encoder\n","    input_tensor = input_tensor.type(torch.FloatTensor)\n","    target_tensor= target_tensor.type(torch.FloatTensor)\n","    input_tensor_length = len(input_tensor)\n","    \n","    rnn_model_hidden = [] #this status will be passed to the decoder\n","    for i in range(rnn_model.repeats):\n","      rnn_model_hidden.append(rnn_model.initHidden(batch_size))\n","\n","    encoder_outputs = torch.zeros([configuration['input_sequence_len'],input_tensor_length], dtype=torch.float32).to(device)\n","\n","    for ei in range(configuration['input_sequence_len']):\n","      #target_tensor_seq = target_tensor.view(configuration['output_sequence_len'], batch_size, configuration['yhat_size'])\n","      input_tensor_seq = input_tensor.view(configuration['input_sequence_len'],batch_size, configuration['feature_len'])[ei]\n","      input_tensor_seq = input_tensor_seq.view(1, batch_size, configuration['feature_len']).to(device)\n","      rnn_model_output, rnn_model_hidden = rnn_model(\n","          input_tensor_seq, rnn_model_hidden)\n","      encoder_outputs[ei]= rnn_model_output.squeeze()\n","\n","    #pass through the decoder  \n","    validation_loss = 0\n","    encoder_outputs = torch.transpose(encoder_outputs, 0, 1).unsqueeze(0)\n","    decoder_output, decoder_hidden = decoder_model(encoder_outputs, rnn_model_hidden)\n","    target_tensor_seq= target_tensor.view(configuration['output_sequence_len'], batch_size, configuration['yhat_size'])\n","    validation_loss += criterion(decoder_output.squeeze(), target_tensor_seq.squeeze().to(device))\n","      \n","    return validation_loss.item() / configuration['input_sequence_len']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8_m7D8l_WAO","colab_type":"code","outputId":"be9271d8-5a5f-4864-c287-a2d8c1924b4f","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"error","timestamp":1589273656084,"user_tz":-330,"elapsed":11194,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["INPUT_ROOT = PATH+'/input_mix'\n","CLEANED_DIR = '/cleaned'\n","#PROCESSED_DIR = '/processed_seq2seq_'+str(configuration['input_sequence_len'])+'_'+str(configuration['input_sequence_len'])\n","PROCESSED_DIR = \"processed_seq2seq_6_1\"\n","\n","test_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['input_sequence_len'])\n","test_dataset = test_dataset.shuffle()\n","test_dataset = test_dataset[:configuration['training_dataset_length']]\n","test_dataloader = DataLoader(test_dataset,batch_size=configuration['training_batch_size'])\n","training_dataloader = test_dataloader\n","validation_dataloader = test_dataloader\n","\n","#load the latest model for training\n","rnn_model = RNNModel(input_size=21,parameter_sizes=[256,256],repeats=1,output_size=1).to(device)#get_latest_model(PATH+'/models_rnn')\n","decoder_model = DecoderNet(input_size=6,parameter_sizes=[256,256],repeats=1,output_size=1).to(device)\n","rnn_model_optimizer = optim.SGD(rnn_model.parameters(), lr=0.1, momentum=0.9)\n","decoder_model_optimizer = optim.SGD(decoder_model.parameters(), lr=0.1, momentum=0.9)\n","\n","train_models( rnn_model, rnn_model_optimizer , decoder_model, decoder_model_optimizer ,learning_rate = 0.1, epochs = 1,id= str(1) , validation=False)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-dd96beeada73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdecoder_model_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model_optimizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model_optimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-00cd96bb7e68>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer, learning_rate, epochs, id, validation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_configurations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#instantiate the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipping'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_configurations' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GHLM8BnLR9sv","colab_type":"code","outputId":"8fe4db34-3ea0-48b3-e4a0-fc377fc87e92","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1589273664259,"user_tz":-330,"elapsed":2020,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["def generate_repeat_counts(alphas , training_dataset_length,\n","                           enc_repeated_nn_parameters = 132096):\n","  repeats = []\n","  feature_len = configuration['feature_len']\n","  yhat_size = configuration['output_size']\n","  model1 = RNNModel(**{\n","         'input_size': configuration['feature_len'],\n","         'parameter_sizes': [256, 256] ,\n","         'repeats' : 1 ,\n","         'output_size' : yhat_size\n","        })\n","  model2 = RNNModel(**{\n","         'input_size': configuration['feature_len'],\n","         'parameter_sizes': [256, 256] ,\n","         'repeats' : 2 ,\n","         'output_size' : yhat_size\n","        }\n","  )\n","\n","  enc_repeated_nn_parameters = get_n_params(model2) - get_n_params(model1)\n","  print(enc_repeated_nn_parameters)\n","\n","  for alpha in alphas:\n","    preferred_parameters  = (1/alpha)* (training_dataset_length / (configuration['feature_len']+configuration['yhat_size']))\n","    enc_repeats = int(\n","        (preferred_parameters - \n","        (preferred_parameters%enc_repeated_nn_parameters))\n","        / enc_repeated_nn_parameters)\n","    for j in range(1, enc_repeats+1):\n","      if j not in repeats:\n","        repeats.append(j)\n","  return repeats\n","\n","def generate_model_parameters(repeat_counts):\n","  model_dict = dict()\n","  model_id = 0\n","  feature_len = configuration['feature_len']\n","  output_size = configuration['output_size']\n","\n","  for enc_repeat in repeat_counts:\n","    model_dict[model_id] = [{\n","         'input_size': feature_len,\n","         'parameter_sizes': [256, 256] ,\n","         'repeats' : enc_repeat ,\n","         'output_size' : output_size\n","        }\n","        ]\n","    model_id+=1\n","  return model_dict\n","\n","def save_model_parameters(model_parameters_dict, models = None, fileName=None):\n","  if fileName == None:\n","    fileName = 'model_parameters_'+str(np.round(np.random.randn(1000,200)[0][0]*1000))\n","\n","  model_summary = pd.DataFrame(columns =[])\n","  for i in sorted(model_parameters_dict.keys()):\n","    param_size = 0\n","    if models != None:\n","      param_size = get_n_params(models[i][0])\n","      param_size += get_n_params(models[i][1])\n","    row = pd.Series({\n","                        'model_id':i,\n","                        'repeats' : model_parameters_dict[i][0]['repeats'],\n","                        'parameters' :param_size\n","    })\n","    row_df = pd.DataFrame([row], index = [i])\n","    model_summary = pd.concat([model_summary, row_df])\n","  model_summary.to_csv(fileName)\n","\n","#Create a new directory to store model results\n","MODEL_PREFIX = '_rnn'\n","MODEL_DIR =  PATH + '/models'+MODEL_PREFIX\n","dirs = []\n","for f in listdir(MODEL_DIR):\n","  if \"run\" in f:\n","    dirs.append(int(f[3]))\n","if len(dirs) > 0:\n","  if os.path.exists(MODEL_DIR+\"/run\"+str(max(dirs)+1)) == False:\n","    os.mkdir(MODEL_DIR+\"/run\"+str(max(dirs)+1))\n","    MODEL_DIR = MODEL_DIR+\"/run\"+str(max(dirs)+1)\n","else:\n","  os.mkdir(MODEL_DIR+\"/run0\")\n","  MODEL_DIR = MODEL_DIR+\"/run0\"\n","\n","if os.path.exists(MODEL_DIR)==False:\n","  os.mkdir(MODEL_DIR)\n","\n","session_id =str(1001)+ '_'+str(datetime.now(tz=None).microsecond)\n","repeat_counts = generate_repeat_counts([1,2,3], 100000000)\n","print(repeat_counts)\n","\n","model_configurations = generate_model_parameters(repeat_counts)\n","save_model_parameters(model_configurations, fileName= MODEL_DIR + '/rnn_model_configurations_'+session_id+'.csv')\n","models = dict()\n","print(len(model_configurations))\n","print(len(repeat_counts))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["526336\n","[1, 2, 3, 4, 5, 6, 7, 8]\n","8\n","8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W34bK4bcqCMi","colab_type":"code","colab":{}},"source":["#Run all the candidate models\n","def train_models( rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer,learning_rate = 0.001, epochs = 2500, id= \"0\",  validation=True):\n","  for i in model_configurations:\n","    #instantiate the models\n","    if i != 1 :\n","      print('skipping', i)\n","      continue\n","    current_model_dir =  MODEL_DIR +'/'+str(i)\n","    if os.path.exists(current_model_dir) == False:\n","      os.mkdir(current_model_dir)\n","    \n","    criterion = nn.MSELoss()\n","\n","    losses ,rnn_model , rnn_model_optimizer, decoder_model, decoder_model_optimizer = trainValidationIters(\n","        training_dataloader,validation_dataloader,\n","        rnn_model,rnn_model_optimizer, \n","        decoder_model,decoder_model_optimizer,\n","        criterion, n_iters = epochs,output_sequence_len=configuration['output_sequence_len'],\n","        input_sequence_len=configuration['input_sequence_len'],\n","        current_model_dir=current_model_dir,  model_id = i,session_id = session_id,\n","        print_every=25, save_every = 100,check_validation_every=50, id=id, validation=validation)\n","    \n","    print('Model ',str(i), ': trained and evaluated')\n","    break\n","    \n","    #save_dict({\n","    #    'model' : rnn_model.state_dict(),\n","    #    'losses' : losses\n","    #}, current_model_dir+'/'+str(i)+'models_'+session_id+'.pt')\n","\n","    #models[i] = {\n","    #    'rnn_model' : rnn_model.state_dict(),\n","    #    'losses' : losses\n","    #}\n","\n","  #save_dict(models, MODEL_DIR+'/models_'+session_id+'.pt')\n","  return rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UewHP07oPqE","colab_type":"code","outputId":"a784e6ce-55e4-48c9-b6b1-ee4a38c4371f","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589273668196,"user_tz":-330,"elapsed":1319,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["#INPUT_ROOT = PATH+'/input_mix'\n","#CLEANED_DIR = '/cleaned'\n","#PROCESSED_DIR = '/processed_seq2seq_'+str(configuration['input_sequence_len'])+'_'+str(configuration['input_sequence_len'])\n","\n","#validation_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['output_sequence_len'])\n","#validation_dataset = validation_dataset.shuffle()\n","#validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n","len(validation_dataset)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"jI2ISDr1GMqL","colab_type":"code","outputId":"6fe70e0a-40d5-40f8-b7bd-1a35dc69c12b","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1589287151592,"user_tz":-330,"elapsed":4855664,"user":{"displayName":"Nikah Proposal","photoUrl":"","userId":"15484371292599858444"}}},"source":["INPUT_ROOT = PATH+'/input_mix'\n","CLEANED_DIR = '/cleaned'\n","PROCESSED_DIR = '/processed_seq2seq_'+str(configuration['input_sequence_len'])+'_'+str(configuration['input_sequence_len'])\n","\n","#validation_dataset = Seq2SeqDataSet(INPUT_ROOT+'/validation', configuration['input_sequence_len'], configuration['output_sequence_len'])\n","#validation_dataset = validation_dataset.shuffle()\n","#validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n","learning_rate = 0.001\n","\n","rnn_model = RNNModel(input_size=21,parameter_sizes=[128,256],repeats=1,output_size=1).to(device)\n","decoder_model = DecoderNet(input_size=6,parameter_sizes=[128,256],repeats=1,output_size=1).to(device)\n","rnn_model_optimizer = optim.SGD(rnn_model.parameters(), lr=learning_rate, momentum=0.9)\n","decoder_model_optimizer = optim.SGD(decoder_model.parameters(), lr=learning_rate, momentum=0.9)\n","\n","dirs = ['/'+f for f in listdir(INPUT_ROOT+'/training') if \"cleaned_non_window\" in f]\n","dirs.reverse()\n","print(dirs)\n","\n","i = 0\n","for CLEANED_DIR  in dirs:\n","  PROCESSED_DIR = CLEANED_DIR.split( sep=\"_\")[0]+'_seq2seq_' +str(configuration['input_sequence_len'])+'_'+str(configuration['output_sequence_len'])\n","  print(PROCESSED_DIR)\n","  \n","  training_dataset = Seq2SeqDataSet(INPUT_ROOT+'/training', configuration['input_sequence_len'], configuration['input_sequence_len'])\n","  training_dataset = training_dataset.shuffle()\n","  training_dataset = training_dataset[:configuration['training_dataset_length']]\n","  training_dataloader = DataLoader(training_dataset,batch_size=configuration['training_batch_size'])\n","\n","  rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer =  train_models( \n","      rnn_model,rnn_model_optimizer,\n","      decoder_model=decoder_model, decoder_model_optimizer=decoder_model_optimizer ,learning_rate = learning_rate, epochs = 1000,id= str(i) )\n","  \n","  torch.save(rnn_model, PATH+'/models_rnn'+CLEANED_DIR+'_rnn.model')\n","  torch.save(rnn_model_optimizer, PATH+'/models_rnn'+CLEANED_DIR+'_rnn.optim')\n","  torch.save(decoder_model, PATH+'/models_rnn'+CLEANED_DIR+'_decoder.model')\n","  torch.save(decoder_model_optimizer, PATH+'/models_rnn'+CLEANED_DIR+'_decoder.optim')\n","  i+=1"],"execution_count":29,"outputs":[{"output_type":"stream","text":["['/2000_cleaned_non_window', '/4000_cleaned_non_window', '/6000_cleaned_non_window', '/8000_cleaned_non_window', '/10000_cleaned_non_window', '/12000_cleaned_non_window', '/14000_cleaned_non_window', '/16000_cleaned_non_window', '/18000_cleaned_non_window', '/20000_cleaned_non_window', '/30000_cleaned_non_window', '/40000_cleaned_non_window', '/50000_cleaned_non_window', '/60000_cleaned_non_window', '/70000_cleaned_non_window', '/80000_cleaned_non_window', '/90000_cleaned_non_window', '/200000_cleaned_non_window', '/300000_cleaned_non_window', '/400000_cleaned_non_window', '/500000_cleaned_non_window', '/600000_cleaned_non_window', '/700000_cleaned_non_window', '/800000_cleaned_non_window', '/900000_cleaned_non_window', '/1000000_cleaned_non_window']\n","/2000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 23s) (25 2%) training:0.000082328511404 validation:0.001207872122382\n","train/validation difference :  0.00112554361097776\n","3m 43s (- 70m 39s) (50 5%) training:0.000082313730200 validation:0.001207922766174\n","train/validation difference :  0.0011256090359745652\n","-8.412317377356566e-07\n","[0.0012088036479581381, 0.0012079624162204025]\n","5m 34s (- 68m 43s) (75 7%) training:0.000082311804460 validation:0.001207993065102\n","train/validation difference :  0.001125681260641811\n","7m 25s (- 66m 50s) (100 10%) training:0.000082310096065 validation:0.001208052612157\n","train/validation difference :  0.0011257425160924057\n","1.0725956608121243e-07\n","[0.0012079647197815531, 0.0012080719793476345]\n","m :  1.0725956608121243e-07\n","Terminating the training at : 100\n","Model  1 : trained and evaluated\n","/4000_seq2seq_6_1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type DecoderNet. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["skipping 0\n","1m 51s (- 72m 23s) (25 2%) training:0.000080543564764 validation:0.001207734219694\n","train/validation difference :  0.001127190654930405\n","3m 42s (- 70m 28s) (50 5%) training:0.000080543564764 validation:0.001207734219694\n","train/validation difference :  0.001127190654930405\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/6000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 29s) (25 2%) training:0.000081915227687 validation:0.001207734219694\n","train/validation difference :  0.001125818992007309\n","3m 41s (- 70m 7s) (50 5%) training:0.000081915227687 validation:0.001207734219694\n","train/validation difference :  0.001125818992007309\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/8000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 71m 44s) (25 2%) training:0.000081685108550 validation:0.001207734219694\n","train/validation difference :  0.0011260491111440918\n","3m 41s (- 70m 1s) (50 5%) training:0.000081685108550 validation:0.001207734219694\n","train/validation difference :  0.0011260491111440918\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/10000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 9s) (25 2%) training:0.000081271194479 validation:0.001207734219694\n","train/validation difference :  0.0011264630252145178\n","3m 42s (- 70m 19s) (50 5%) training:0.000081271194479 validation:0.001207734219694\n","train/validation difference :  0.0011264630252145178\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/12000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 27s) (25 2%) training:0.000081764622943 validation:0.001207734219694\n","train/validation difference :  0.0011259695967508\n","3m 43s (- 70m 43s) (50 5%) training:0.000081764622943 validation:0.001207734219694\n","train/validation difference :  0.0011259695967508\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/14000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 30s) (25 2%) training:0.000082561983618 validation:0.001207734219694\n","train/validation difference :  0.0011251722360761279\n","3m 42s (- 70m 29s) (50 5%) training:0.000082561983618 validation:0.001207734219694\n","train/validation difference :  0.0011251722360761279\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/16000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 72m 3s) (25 2%) training:0.000081299242690 validation:0.001207734219694\n","train/validation difference :  0.0011264349770042448\n","3m 41s (- 70m 8s) (50 5%) training:0.000081299242690 validation:0.001207734219694\n","train/validation difference :  0.0011264349770042448\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/18000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 72m 5s) (25 2%) training:0.000081485892527 validation:0.001207734219694\n","train/validation difference :  0.0011262483271667332\n","3m 42s (- 70m 25s) (50 5%) training:0.000081485892527 validation:0.001207734219694\n","train/validation difference :  0.0011262483271667332\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/20000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 71m 59s) (25 2%) training:0.000080897158417 validation:0.001207734219694\n","train/validation difference :  0.0011268370612770849\n","3m 41s (- 70m 9s) (50 5%) training:0.000080897158417 validation:0.001207734219694\n","train/validation difference :  0.0011268370612770849\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/30000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 71m 57s) (25 2%) training:0.000081002744385 validation:0.001207734219694\n","train/validation difference :  0.001126731475308672\n","3m 41s (- 70m 16s) (50 5%) training:0.000081002744385 validation:0.001207734219694\n","train/validation difference :  0.001126731475308672\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/40000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 16s) (25 2%) training:0.000082724025560 validation:0.001207734219694\n","train/validation difference :  0.001125010194133597\n","3m 42s (- 70m 20s) (50 5%) training:0.000082724025560 validation:0.001207734219694\n","train/validation difference :  0.001125010194133597\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/50000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 72m 4s) (25 2%) training:0.000080997418081 validation:0.001207734219694\n","train/validation difference :  0.0011267368016128103\n","3m 41s (- 70m 11s) (50 5%) training:0.000080997418081 validation:0.001207734219694\n","train/validation difference :  0.0011267368016128103\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/60000_seq2seq_6_1\n","skipping 0\n","1m 50s (- 72m 1s) (25 2%) training:0.000082103844155 validation:0.001207734219694\n","train/validation difference :  0.001125630375539087\n","3m 41s (- 70m 7s) (50 5%) training:0.000082103844155 validation:0.001207734219694\n","train/validation difference :  0.001125630375539087\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/70000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 24s) (25 2%) training:0.000082301897540 validation:0.001207734219694\n","train/validation difference :  0.0011254323221538396\n","3m 43s (- 70m 47s) (50 5%) training:0.000082301897540 validation:0.001207734219694\n","train/validation difference :  0.0011254323221538396\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/80000_seq2seq_6_1\n","skipping 0\n","1m 51s (- 72m 39s) (25 2%) training:0.000082328156168 validation:0.001207734219694\n","train/validation difference :  0.0011254060635259749\n","3m 44s (- 71m 3s) (50 5%) training:0.000082328156168 validation:0.001207734219694\n","train/validation difference :  0.0011254060635259749\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/90000_seq2seq_6_1\n","skipping 0\n","1m 52s (- 73m 1s) (25 2%) training:0.000082941336586 validation:0.001207734219694\n","train/validation difference :  0.0011247928831075343\n","3m 45s (- 71m 17s) (50 5%) training:0.000082941336586 validation:0.001207734219694\n","train/validation difference :  0.0011247928831075343\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/200000_seq2seq_6_1\n","skipping 0\n","1m 52s (- 73m 6s) (25 2%) training:0.000080111256163 validation:0.001207734219694\n","train/validation difference :  0.0011276229635311741\n","3m 44s (- 71m 6s) (50 5%) training:0.000080111256163 validation:0.001207734219694\n","train/validation difference :  0.0011276229635311741\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/300000_seq2seq_6_1\n","skipping 0\n","1m 52s (- 72m 48s) (25 2%) training:0.000081478096339 validation:0.001207734219694\n","train/validation difference :  0.0011262561233553168\n","3m 43s (- 70m 42s) (50 5%) training:0.000081478096339 validation:0.001207734219694\n","train/validation difference :  0.0011262561233553168\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/400000_seq2seq_6_1\n","skipping 0\n","1m 52s (- 72m 59s) (25 2%) training:0.000081906154264 validation:0.001207734219694\n","train/validation difference :  0.0011258280654296187\n","3m 44s (- 70m 59s) (50 5%) training:0.000081906154264 validation:0.001207734219694\n","train/validation difference :  0.0011258280654296187\n","4.44331997762615e-20\n","[0.0012077342196939072, 0.0012077342196939072]\n","m :  4.44331997762615e-20\n","Terminating the training at : 50\n","Model  1 : trained and evaluated\n","/500000_seq2seq_6_1\n","skipping 0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-f897eda5f04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer =  train_models( \n\u001b[1;32m     30\u001b[0m       \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_model_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       decoder_model=decoder_model, decoder_model_optimizer=decoder_model_optimizer ,learning_rate = learning_rate, epochs = 1000,id= str(i) )\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/models_rnn'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mCLEANED_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_rnn.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-4fdc27647684>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer, learning_rate, epochs, id, validation)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_sequence_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcurrent_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_model_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         print_every=25, save_every = 100,check_validation_every=50, id=id, validation=validation)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m': trained and evaluated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-1deab5ba5410>\u001b[0m in \u001b[0;36mtrainValidationIters\u001b[0;34m(train_dataloader, validation_dataloader, rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer, criterion, n_iters, output_sequence_len, input_sequence_len, current_model_dir, model_id, session_id, print_every, plot_every, save_every, check_validation_every, id, validation)\u001b[0m\n\u001b[1;32m     52\u001b[0m       training_loss = train_batch(train_dataloader, \n\u001b[1;32m     53\u001b[0m                                   \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_model_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                   decoder_model, decoder_model_optimizer, criterion )\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-1deab5ba5410>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(dataloader, rnn_model, rnn_model_optimizer, decoder_model, decoder_model_optimizer, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m                 decoder_model, decoder_model_optimizer,criterion):\n\u001b[1;32m      6\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss += (train_rnn(batch.x.view(batch_size, configuration['input_sequence_len'] , configuration['feature_len']), batch.y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/dataloader.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         super(DataLoader,\n\u001b[1;32m     46\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                              collate_fn=lambda batch: collate(batch), **kwargs)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/dataloader.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(data_list, follow_batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m         :obj:`follow_batch`.\"\"\"\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'batch'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         :obj:`follow_batch`.\"\"\"\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'batch'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;34mr\"\"\"Returns all names of graph attributes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'__'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"K-1dxnG1nl6s","colab_type":"code","colab":{}},"source":["validation_dataset[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zk5WWsBe43Jz","colab_type":"code","colab":{}},"source":["import copy\n","\n","def train_batch(dataloader, \n","                rnn_model,rnn_model_optimizer, \n","                decoder_model, decoder_model_optimizer,criterion):\n","  loss = 0\n","  for batch in dataloader:\n","    batch_size = int(len(batch.x)/configuration['input_sequence_len'])\n","    loss += (train_rnn(batch.x.view(batch_size, configuration['input_sequence_len'] , configuration['feature_len']), batch.y,\n","                       rnn_model,rnn_model_optimizer,\n","                       decoder_model, decoder_model_optimizer,\n","                       criterion, batch_size))/ batch_size\n","  return loss / len(dataloader)\n","\n","def batch_evaluate(dataloader, \n","                   rnn_model,rnn_model_optimizer,\n","                   decoder_model, decoder_model_optimizer, criterion, output_sequence_len):\n","  loss = 0\n","  for batch in dataloader:\n","    batch_size = int(len(batch.x)/configuration['input_sequence_len'])\n","    loss += (evaluate_rnn(batch.x.view(batch_size, configuration['input_sequence_len'] , configuration['feature_len']), batch.y,\n","                          rnn_model,rnn_model_optimizer,\n","                          decoder_model, decoder_model_optimizer,\n","                          criterion, batch_size))/ batch_size\n","  return loss / len(dataloader)\n","\n","def trainValidationIters(train_dataloader,validation_dataloader,\n","                         rnn_model, rnn_model_optimizer,\n","                         decoder_model, decoder_model_optimizer, \n","                        criterion ,n_iters, output_sequence_len,input_sequence_len,\n","                        current_model_dir, model_id,session_id,print_every=1000, plot_every=100,save_every = 100,check_validation_every=5,id=\"train_id_\", validation=True):\n","\n","  start = time.time()\n","  plot_losses = []\n","  \n","  # Reset every print_every # Reset every plot_every\n","  print_training_loss_total = 0\n","  plot_training_loss_total = 0\n","  print_validation_loss_total = 0\n","  plot_validation_loss_total = 0\n","  save_validation_loss_total = 0\n","  save_training_loss_total = 0\n","      \n","  losses = []\n","  rnn_models_temp = dict()\n","  rnn_optim_temp = dict()\n","  decoder_models_temp = dict()\n","  decoder_optim_temp = dict()\n","  for iter in range(1, n_iters + 1):\n","      decoder_model.train()\n","      rnn_model.train()\n","      training_loss = train_batch(train_dataloader, \n","                                  rnn_model,rnn_model_optimizer,\n","                                  decoder_model, decoder_model_optimizer, criterion )\n","      \n","      if validation == True:\n","        rnn_model.eval()\n","        decoder_model.eval()\n","        validation_loss =  batch_evaluate(validation_dataloader, \n","                                          rnn_model,rnn_model_optimizer,\n","                                          decoder_model, decoder_model_optimizer, \n","                                          criterion,configuration['output_sequence_len'] )\n","      else:\n","        validation_loss = 0\n","\n","      rnn_models_temp[validation_loss] = copy.deepcopy(rnn_model) #Save the model\n","      rnn_optim_temp[validation_loss] = copy.deepcopy(rnn_model_optimizer) #Save the model\n","      decoder_models_temp[validation_loss] = copy.deepcopy(decoder_model) #Save the model\n","      decoder_optim_temp[validation_loss] = copy.deepcopy(decoder_model_optimizer) #Save the model\n","\n","      print_training_loss_total += training_loss\n","      plot_training_loss_total += training_loss\n","      print_validation_loss_total += validation_loss\n","      plot_validation_loss_total += validation_loss\n","      \n","      save_training_loss_total += training_loss\n","      save_validation_loss_total += validation_loss\n","      \n","      if iter % print_every == 0:\n","          print_training_loss_avg = print_training_loss_total / print_every\n","          print_training_loss_total = 0\n","          print_validation_loss_avg = print_validation_loss_total / print_every\n","          print_validation_loss_total = 0\n","          print('%s (%d %d%%) training:%.15f validation:%.15f' % (timeSince(start, iter / n_iters),\n","                                        iter, iter / n_iters * 100, print_training_loss_avg,print_validation_loss_avg, ))\n","          print('train/validation difference : ',print_validation_loss_avg-print_training_loss_avg)\n","          current_model = rnn_model\n","          current_optimizer = decoder_model_optimizer\n","        \n","      if iter % save_every == 0:\n","        save_training_loss_avg = save_training_loss_total / save_every\n","        save_validation_loss_avg = plot_validation_loss_total / save_every\n","        #'arch': args.arch,\n","        save_checkpoint({\n","            'epoch': iter + 1,\n","            'rnn_model_state_dict': rnn_model.state_dict(),\n","            'training_loss': save_training_loss_avg,\n","            'validation_loss': save_validation_loss_avg,\n","        }, False, filename=current_model_dir+'/'+str(iter)+'_'+str(session_id)+'.pt')\n","\n","      losses.append([training_loss, validation_loss])\n","\n","      if iter % check_validation_every == 0 and iter >= check_validation_every:\n","        #x = [i for i in range(1,check_validation_every+1)]\n","        x = [0 , 1]\n","        y = np.array(losses).T[1][-1*check_validation_every:].tolist()\n","        y = [y[-1*check_validation_every:][0], y[-1*check_validation_every:][-1]]\n","        m,b = np.polyfit(x, y, 1)\n","        print(m)\n","        print(y)\n","        if m >= -1*10**-9 :\n","          print('m : ', m)\n","          print('Terminating the training at :',iter )\n","          break\n","  \n","  for k in sorted(rnn_models_temp.keys()):\n","    rnn_model = rnn_models_temp[k]\n","    rnn_model_optimizer = rnn_optim_temp[k]\n","    break\n","\n","  for k in sorted(decoder_models_temp.keys()):\n","    decoder_model = decoder_models_temp[k]\n","    decoder_model_optimizer = decoder_optim_temp[k]\n","    break\n","\n","  save_dict({\n","      'losses' : losses\n","  }, filename=current_model_dir+'/losses_'+id)\n","\n","  return losses , rnn_model , rnn_model_optimizer, decoder_model, decoder_model_optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7Cc0eDOahyt","colab_type":"code","colab":{}},"source":["#los = [1,2,3,4,5,5,6,7,87,9]\n","#print(los[-3:][-1])\n","#print(los[-3:][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbZSFpPGX215","colab_type":"code","colab":{}},"source":["\"\"\"\n","Common function\n","\"\"\"\n","def get_latest_model(mypath):\n","  model = RNNModel(**model_configurations[4][0])\n","  fnames= []\n","  for fname in listdir(mypath):\n","    if \"cleaned_non_window\" in fname:\n","      fnames.append(int(fname.split('_')[0]))\n","  print('Model path: ', mypath+'/'+str(min(fnames))+'_cleaned_non_window.model')\n","\n","  if torch.cuda.is_available():\n","    print('cuda rnn_model')\n","    return torch.load(mypath+'/'+str(min(fnames))+'_cleaned_non_window.model')\n","  else:\n","    print('cpu rnn_model')\n","    return torch.load(mypath+'/'+str(min(fnames))+'_cleaned_non_window.model',  map_location=torch.device('cpu'))\n","  return model\n","\n","def get_n_params(model):\n","    pp=0\n","    for p in list(model.parameters()):\n","        nn=1\n","        for s in list(p.size()):\n","            nn = nn*s\n","        pp += nn\n","    return pp\n","  \n","\n","def save_model_parameters(model_parameters_dict, models = None, fileName=None):\n","  if fileName == None:\n","    fileName = 'model_parameters_'+str(np.round(np.random.randn(1000,200)[0][0]*1000))\n","\n","  model_summary = pd.DataFrame(columns =[])\n","  for i in sorted(model_parameters_dict.keys()):\n","    param_size = 0\n","    if models != None:\n","      param_size = get_n_params(models[i][0])\n","      param_size += get_n_params(models[i][1])\n","    row = pd.Series({\n","                        'model_id':i,\n","                        'encoder repeats' : model_parameters_dict[i][0]['repeats'],\n","                        'decoder repeats' : model_parameters_dict[i][1]['repeats'],\n","                        'parameters' :param_size\n","    })\n","    row_df = pd.DataFrame([row], index = [i])\n","    model_summary = pd.concat([model_summary, row_df])\n","  model_summary.to_csv(fileName)\n","\n","#Generate the list of repeat count parameters list\n","\n","#generate_model_parameters(generate_repeat_counts())\n","\n","def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n","    torch.save(state, filename)\n","    if is_best:\n","        shutil.copyfile(filename, 'model_best.pth.tar')\n","\n","def save_dict(dictionary, filename):\n","  np.save(filename, dictionary)\n","\n","import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points, points2):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.xlabel('Iteration')\n","    plt.ylabel('Loss')\n","    plt.plot(points)\n","    plt.plot(points2)\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijYfN733wE21","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}